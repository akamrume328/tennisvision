import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, List, Dict, Any # <- Dict, Any ã‚’è¿½åŠ 
import re # æ­£è¦è¡¨ç¾ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import json # JSONãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

class MergeFeaturesAndPredictions:
    """
    æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSV (feature_extractor.pyå‡ºåŠ›) ã¨ LSTMäºˆæ¸¬CSV (predict_lstm_model.pyå‡ºåŠ›) ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    ãƒãƒ¼ã‚¸æ™‚ã«ã€HMMå­¦ç¿’ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ç”Ÿæˆãƒ»ä¿å­˜ã—ã¾ã™ã€‚
    """
    def __init__(self, 
                 output_dir_name: str = "merged_predictions",
                 base_dir: Path = Path("./training_data")):
        """
        MergeFeaturesAndPredictions ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Args:
            output_dir_name (str): ãƒãƒ¼ã‚¸ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã€‚
            base_dir (Path): `training_data` ãªã©ã®åŸºæº–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        """
        self.base_dir = base_dir
        self.output_dir = self.base_dir / output_dir_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ãƒãƒ¼ã‚¸çµæœã®ä¿å­˜å…ˆ: {self.output_dir.resolve()}")
        print(f"HMMç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ä¸Šè¨˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    def _select_file_dialog(self, title: str, directory: Path, pattern: str) -> Optional[Path]:
        """
        ç°¡æ˜“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ï¼‰ã€‚
        æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸æŠã‚’ä¿ƒã—ã¾ã™ã€‚
        """
        print(f"\n=== {title} ===")
        if not directory.exists():
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {directory}")
            return None
            
        files = sorted(list(directory.glob(pattern)))
        if not files:
            print(f"âŒ {pattern} ã«ä¸€è‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {directory}")
            return None

        for i, f_path in enumerate(files, 1):
            try:
                mtime = datetime.fromtimestamp(f_path.stat().st_mtime)
                print(f"  {i}. {f_path.name} (æ›´æ–°æ—¥æ™‚: {mtime:%Y-%m-%d %H:%M})")
            except FileNotFoundError:
                print(f"  {i}. {f_path.name} (ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚)")
                continue # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        
        if not any(f.exists() for f in files): # æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸€ã¤ã‚‚ãªã‘ã‚Œã°çµ‚äº†
            print(f"âŒ æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸€ã¤ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None

        while True:
            try:
                choice_str = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(files)}), 0ã§ã‚¹ã‚­ãƒƒãƒ—: ").strip()
                choice_num = int(choice_str)
                if choice_num == 0:
                    return None
                if 1 <= choice_num <= len(files):
                    selected_file = files[choice_num - 1]
                    if selected_file.exists():
                        return selected_file
                    else:
                        print(f"é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ« {selected_file.name} ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚å†åº¦é¸æŠã—ã¦ãã ã•ã„ã€‚")
                else:
                    print(f"ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚1ã‹ã‚‰{len(files)}ã®é–“ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            except ValueError:
                print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            except IndexError:
                print("é¸æŠè‚¢ã®ç¯„å›²å¤–ã§ã™ã€‚")


    def load_csv(self, file_path: Path, file_description: str) -> Optional[pd.DataFrame]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Args:
            file_path (Path): èª­ã¿è¾¼ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
            file_description (str): ãƒ•ã‚¡ã‚¤ãƒ«ã®èª¬æ˜ï¼ˆä¾‹: "æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSV"ï¼‰ã€‚

        Returns:
            Optional[pd.DataFrame]: èª­ã¿è¾¼ã¾ã‚ŒãŸDataFrameã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã€‚
        """
        if not file_path or not file_path.exists():
            print(f"âŒ {file_description}ã®ãƒ‘ã‚¹ãŒç„¡åŠ¹ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
            return None
        try:
            df = pd.read_csv(file_path)
            print(f"âœ… {file_description}ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path.name} ({len(df)}è¡Œ)")
            return df
        except Exception as e:
            print(f"âŒ {file_description}ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({file_path.name}): {e}")
            return None

    def merge_data(self, 
                   teacher_df: pd.DataFrame, 
                   prediction_df: pd.DataFrame,
                   teacher_true_label_col: str,
                   prediction_pred_label_col: str,
                   merge_keys: Optional[List[str]] = None
                   ) -> Optional[pd.DataFrame]:
        """
        æ•™å¸«ãƒ‡ãƒ¼ã‚¿DataFrameã¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿DataFrameã‚’ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚

        Args:
            teacher_df (pd.DataFrame): æ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼ˆçœŸã®ãƒ©ãƒ™ãƒ«ã‚’å«ã‚€ï¼‰ã€‚
            prediction_df (pd.DataFrame): LSTMäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã€‚
            teacher_true_label_col (str): æ•™å¸«ãƒ‡ãƒ¼ã‚¿DFå†…ã®çœŸã®ãƒ©ãƒ™ãƒ«åˆ—åã€‚
            prediction_pred_label_col (str): äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿DFå†…ã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ—åã€‚
            merge_keys (Optional[List[str]]): ãƒãƒ¼ã‚¸ã«ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼åˆ—åã®ãƒªã‚¹ãƒˆã€‚
                                            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ ['video_name', 'frame_number']ã€‚

        Returns:
            Optional[pd.DataFrame]: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸDataFrameã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã€‚
        """
        if merge_keys is None:
            merge_keys = ['video_name', 'frame_number']

        print(f"\n--- ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸é–‹å§‹ ---")
        print(f"ãƒãƒ¼ã‚¸ã‚­ãƒ¼: {merge_keys}")
        print(f"æ•™å¸«ãƒ‡ãƒ¼ã‚¿ çœŸãƒ©ãƒ™ãƒ«åˆ—: {teacher_true_label_col}")
        print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ—: {prediction_pred_label_col}")

        # ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨ãƒ‡ãƒ¼ã‚¿å‹èª¿æ•´
        for key in merge_keys:
            if key not in teacher_df.columns:
                print(f"âŒ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ã‚­ãƒ¼ '{key}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                return None
            if key not in prediction_df.columns:
                print(f"âŒ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ã‚­ãƒ¼ '{key}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’çµ±ä¸€ã™ã‚‹è©¦ã¿
            try:
                if 'frame_number' in key.lower() or 'index' in key.lower(): # ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‰ã—ãåˆ—ã¯æ•´æ•°å‹ã«
                    print(f"â„¹ï¸ ãƒãƒ¼ã‚¸ã‚­ãƒ¼ '{key}' ã‚’æ•´æ•°å‹ã«å¤‰æ›ã—ã¾ã™ã€‚")
                    teacher_df[key] = pd.to_numeric(teacher_df[key], errors='coerce').astype('Int64') # Nullable Integer
                    prediction_df[key] = pd.to_numeric(prediction_df[key], errors='coerce').astype('Int64')
                elif 'name' in key.lower() or 'video' in key.lower(): # åå‰ã‚„ãƒ“ãƒ‡ã‚ªåã‚‰ã—ãåˆ—ã¯æ–‡å­—åˆ—å‹ã«
                    print(f"â„¹ï¸ ãƒãƒ¼ã‚¸ã‚­ãƒ¼ '{key}' ã‚’æ–‡å­—åˆ—å‹ã«å¤‰æ›ã—ã€å‰å¾Œã®ç©ºç™½ã‚’é™¤å»ã—ã¾ã™ã€‚")
                    teacher_df[key] = teacher_df[key].astype(str).str.strip()
                    prediction_df[key] = prediction_df[key].astype(str).str.strip()
                    
                    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã® video_name ã‹ã‚‰æ—¥ä»˜ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ (_YYYYMMDD) ã‚’é™¤å»
                    if key == 'video_name':
                        print(f"â„¹ï¸ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã® '{key}' ã‹ã‚‰æ—¥ä»˜ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ã—ã¾ã™ã€‚")
                        # ä¾‹: "video1_20230101" -> "video1"
                        prediction_df[key] = prediction_df[key].apply(lambda x: re.sub(r'_\d{8}$', '', x))
                
                # å¤‰æ›å¾Œã«NaNãŒå¤šæ•°ç™ºç”Ÿã—ã¦ã„ãªã„ã‹ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                if teacher_df[key].isnull().sum() > 0.5 * len(teacher_df): # åŠæ•°ä»¥ä¸ŠNaNãªã‚‰è­¦å‘Š
                    print(f"âš ï¸ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ '{key}' ã«å¤‰æ›å¾ŒNaNãŒå¤šæ•°ã‚ã‚Šã¾ã™ã€‚å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                if prediction_df[key].isnull().sum() > 0.5 * len(prediction_df):
                    print(f"âš ï¸ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ '{key}' ã«å¤‰æ›å¾ŒNaNãŒå¤šæ•°ã‚ã‚Šã¾ã™ã€‚å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

            except Exception as e:
                print(f"âŒ ãƒãƒ¼ã‚¸ã‚­ãƒ¼ '{key}' ã®ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                return None

        if teacher_true_label_col not in teacher_df.columns:
            print(f"âŒ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«çœŸã®ãƒ©ãƒ™ãƒ«åˆ— '{teacher_true_label_col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return None
        if prediction_pred_label_col not in prediction_df.columns:
            print(f"âŒ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ— '{prediction_pred_label_col}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return None

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º
        print("\n--- ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ (å¤‰æ›å¾Œ) ---")
        for key in merge_keys:
            print(f"æ•™å¸«ãƒ‡ãƒ¼ã‚¿ '{key}' (å‹: {teacher_df[key].dtype}):\n{teacher_df[key].head()}")
            print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ '{key}' (å‹: {prediction_df[key].dtype}):\n{prediction_df[key].head()}")
            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã®æ•°ã‚‚å†ç¢ºèª
            print(f"  æ•™å¸«ãƒ‡ãƒ¼ã‚¿ '{key}' ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {teacher_df[key].nunique()}")
            print(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ '{key}' ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {prediction_df[key].nunique()}")
            # å…±é€šã®ã‚­ãƒ¼ã®å€¤ãŒã‚ã‚‹ã‹ç¢ºèª (æœ€åˆã®ã„ãã¤ã‹)
            common_keys_sample = pd.Series(list(set(teacher_df[key].dropna().unique()) & set(prediction_df[key].dropna().unique()))).head()
            if not common_keys_sample.empty:
                print(f"  '{key}' ã®å…±é€šã®å€¤ (ã‚µãƒ³ãƒ—ãƒ«): {common_keys_sample.tolist()}")
            else:
                print(f"  '{key}' ã«å…±é€šã®å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        
        # video_name ã®è©³ç´°æ¯”è¼ƒ
        if 'video_name' in merge_keys:
            print("\n--- è©³ç´°ãƒ‡ãƒãƒƒã‚°: video_name ã®æ¯”è¼ƒ ---")
            teacher_video_names = set(teacher_df['video_name'].astype(str).unique())
            prediction_video_names = set(prediction_df['video_name'].astype(str).unique())

            common_video_names = sorted(list(teacher_video_names.intersection(prediction_video_names)))
            teacher_only_video_names = sorted(list(teacher_video_names.difference(prediction_video_names)))
            prediction_only_video_names = sorted(list(prediction_video_names.difference(teacher_video_names)))

            print(f"æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿å­˜åœ¨ã™ã‚‹video_name ({len(teacher_only_video_names)}ä»¶): {teacher_only_video_names[:10]}") # å…ˆé ­10ä»¶è¡¨ç¤º
            if len(teacher_only_video_names) > 10: print("  ...")
            print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿å­˜åœ¨ã™ã‚‹video_name ({len(prediction_only_video_names)}ä»¶): {prediction_only_video_names[:10]}")
            if len(prediction_only_video_names) > 10: print("  ...")
            print(f"ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹video_name ({len(common_video_names)}ä»¶): {common_video_names[:10]}")
            if len(common_video_names) > 10: print("  ...")

            if not common_video_names:
                print("âŒ å…±é€šã®video_nameãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚video_nameã®å‘½åè¦å‰‡ã‚„å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                # return None # ã“ã“ã§å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã™
            elif common_video_names and 'frame_number' in merge_keys:
                print("\n--- è©³ç´°ãƒ‡ãƒãƒƒã‚°: å…±é€švideo_nameã«ãŠã‘ã‚‹frame_numberã®æ¯”è¼ƒ (æœ€åˆã®å…±é€švideo_nameã«ã¤ã„ã¦) ---")
                sample_video_name = common_video_names[0]
                
                teacher_sample_frames_df = teacher_df[teacher_df['video_name'] == sample_video_name]
                prediction_sample_frames_df = prediction_df[prediction_df['video_name'] == sample_video_name]

                if teacher_sample_frames_df.empty:
                    print(f"  ãƒ“ãƒ‡ã‚ª '{sample_video_name}': æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                elif prediction_sample_frames_df.empty:
                     print(f"  ãƒ“ãƒ‡ã‚ª '{sample_video_name}': äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    teacher_sample_frames = teacher_sample_frames_df['frame_number']
                    prediction_sample_frames = prediction_sample_frames_df['frame_number']
                    print(f"ãƒ“ãƒ‡ã‚ª: {sample_video_name}")
                    print(f"  æ•™å¸«ãƒ‡ãƒ¼ã‚¿ frame_number (min/max/nunique/type/nulls/sample): {teacher_sample_frames.min()}/{teacher_sample_frames.max()}/{teacher_sample_frames.nunique()}/{teacher_sample_frames.dtype}/{teacher_sample_frames.isnull().sum()}/{teacher_sample_frames.dropna().head().tolist()}")
                    print(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ frame_number (min/max/nunique/type/nulls/sample): {prediction_sample_frames.min()}/{prediction_sample_frames.max()}/{prediction_sample_frames.nunique()}/{prediction_sample_frames.dtype}/{prediction_sample_frames.isnull().sum()}/{prediction_sample_frames.dropna().head().tolist()}")
                    
                    # å…±é€šã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·
                    common_frames = set(teacher_sample_frames.dropna().unique()) & set(prediction_sample_frames.dropna().unique())
                    print(f"  å…±é€šã®frame_number ({len(common_frames)}ä»¶): {sorted(list(common_frames))[:10]}")
                    if len(common_frames) > 10: print("    ...")


        try:
            # äºˆæ¸¬DFã‹ã‚‰ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«åˆ—ã®ã¿ã‚’é¸æŠï¼ˆé‡è¤‡åˆ—ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
            # ãŸã ã—ã€ä»–ã®æœ‰ç”¨ãªåˆ—ï¼ˆä¾‹ï¼šä¿¡é ¼åº¦ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚‚æ®‹ã™ã“ã¨ã‚’æ¤œè¨
            cols_to_keep_from_prediction = merge_keys + [prediction_pred_label_col]
            # prediction_df ã«ä»–ã®æœ‰ç”¨ãªåˆ—ãŒã‚ã‚Œã°ã“ã“ã«è¿½åŠ 
            # ä¾‹: if 'confidence' in prediction_df.columns: cols_to_keep_from_prediction.append('confidence')
            
            # å­˜åœ¨ã—ãªã„åˆ—ã‚’å‚ç…§ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ã€å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠ
            cols_to_keep_from_prediction = [col for col in cols_to_keep_from_prediction if col in prediction_df.columns]
            # ãƒãƒ¼ã‚¸ã‚­ãƒ¼ãŒ cols_to_keep_from_prediction ã«å«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            for key_col in merge_keys:
                if key_col not in cols_to_keep_from_prediction:
                    cols_to_keep_from_prediction.append(key_col)
            cols_to_keep_from_prediction = list(dict.fromkeys(cols_to_keep_from_prediction)) # é‡è¤‡å‰Šé™¤ã—ã¤ã¤é †åºç¶­æŒ

            prediction_subset_df = prediction_df[cols_to_keep_from_prediction].copy()

            # ãƒãƒ¼ã‚¸å®Ÿè¡Œ (inner join ã‚’ä½¿ç”¨ã—ã€ä¸¡æ–¹ã®DFã«å­˜åœ¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿æŒ)
            # HMMã®å¾Œå‡¦ç†ã§ã¯ã€çœŸã®ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®ä¸¡æ–¹ãŒå¿…è¦ã«ãªã‚‹ãŸã‚ã€‚
            merged_df = pd.merge(teacher_df, prediction_subset_df, on=merge_keys, how='inner')
            
            print(f"âœ… ãƒãƒ¼ã‚¸æˆåŠŸ: {len(merged_df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ãŒãƒãƒ¼ã‚¸ã•ã‚Œã¾ã—ãŸã€‚")
            if len(merged_df) == 0:
                print(f"âš ï¸  ãƒãƒ¼ã‚¸å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒ0è¡Œã§ã™ã€‚ãƒãƒ¼ã‚¸ã‚­ãƒ¼ãŒä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                print(f"  æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {[teacher_df[key].nunique() for key in merge_keys]}")
                print(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {[prediction_df[key].nunique() for key in merge_keys]}")


            # å¿…è¦ã«å¿œã˜ã¦åˆ—åã‚’æ¨™æº–åŒ– (ä¾‹: HMMãŒæœŸå¾…ã™ã‚‹åˆ—åã«)
            # ã“ã®ä¾‹ã§ã¯ã€hmm_postprocessor.py ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã«åˆã‚ã›ã¦ãŠã
            # merged_df.rename(columns={teacher_true_label_col: 'true_phase', 
            #                           prediction_pred_label_col: 'predicted_phase'}, 
            #                  inplace=True)
            # print(f"â„¹ï¸ åˆ—åã‚’ãƒªãƒãƒ¼ãƒ : '{teacher_true_label_col}' -> 'true_phase', '{prediction_pred_label_col}' -> 'predicted_phase'")


            return merged_df
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _derive_labels_from_df(self, df: pd.DataFrame, column: str) -> Tuple[Optional[List[str]], Optional[Dict[str, int]], Optional[Dict[int, str]]]:
        """DataFrameã®æŒ‡å®šã•ã‚ŒãŸå˜ä¸€åˆ—ã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡ºã—ã€ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆã™ã‚‹"""
        if column not in df.columns:
            print(f"âš ï¸è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸåˆ— '{column}' ãŒDataFrameã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ãƒ©ãƒ™ãƒ«ã‚’æ´¾ç”Ÿã§ãã¾ã›ã‚“ã€‚")
            return None, None, None
        
        # NaNã‚„ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–
        unique_labels = {label for label in df[column].unique() if pd.notna(label) and str(label).strip() != ""}
        if not unique_labels:
            print(f"âš ï¸è­¦å‘Š: åˆ— '{column}' ã‹ã‚‰æœ‰åŠ¹ãªãƒ©ãƒ™ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None, None, None
            
        sorted_labels = sorted(list(unique_labels))
        label_to_int_map = {label: i for i, label in enumerate(sorted_labels)}
        int_to_label_map = {i: label for i, label in enumerate(sorted_labels)}
        print(f"â„¹ï¸ DataFrameã®åˆ— '{column}' ã‹ã‚‰ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’æ´¾ç”Ÿ: {label_to_int_map}")
        return sorted_labels, label_to_int_map, int_to_label_map

    def _load_teacher_metadata_labels(self, teacher_csv_path: Path) -> Tuple[Optional[List[str]], Optional[Dict[str, int]], Optional[Dict[int, str]]]:
        """æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã‹ã‚‰ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
        if not teacher_csv_path:
            return None, None, None
            
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¨æ¸¬ (ä¾‹: my_features.csv -> my_features_metadata.json)
        metadata_filename = teacher_csv_path.stem + "_metadata.json"
        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã¨ä»®å®š
        potential_metadata_path = teacher_csv_path.parent / metadata_filename
        
        # LSTMãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚æ¢ã™ (ã‚ˆã‚Šä¸€èˆ¬çš„ãª 'phase_labels' ã‚’æŒã¤å¯èƒ½æ€§)
        # ä¾‹: training_data/lstm_models/some_model_set/tennis_pytorch_metadata_*.json
        # ã“ã‚Œã¯ã‚ˆã‚Šè¤‡é›‘ãªæ¢ç´¢ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã«ãªã‚‹ãŸã‚ã€ä»Šå›ã¯æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç›´çµã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å„ªå…ˆ
        
        if potential_metadata_path.exists():
            print(f"â„¹ï¸ æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {potential_metadata_path}")
            try:
                with open(potential_metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # 'phase_labels' ã¾ãŸã¯ 'labels' ã‚­ãƒ¼ã‚’æ¢ã™
                phase_labels = metadata.get('phase_labels') or metadata.get('labels')
                
                if phase_labels and isinstance(phase_labels, list):
                    sorted_labels = sorted(list(set(phase_labels))) # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
                    label_to_int_map = {label: i for i, label in enumerate(sorted_labels)}
                    int_to_label_map = {i: label for i, label in enumerate(sorted_labels)}
                    print(f"âœ… æ•™å¸«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’ãƒ­ãƒ¼ãƒ‰: {label_to_int_map}")
                    return sorted_labels, label_to_int_map, int_to_label_map
                else:
                    print(f"âš ï¸è­¦å‘Š: æ•™å¸«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«æœ‰åŠ¹ãª 'phase_labels' ã¾ãŸã¯ 'labels' (ãƒªã‚¹ãƒˆå½¢å¼) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            except Exception as e:
                print(f"âŒ æ•™å¸«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿/è§£æã‚¨ãƒ©ãƒ¼ ({potential_metadata_path.name}): {e}")
        else:
            print(f"â„¹ï¸æƒ…å ±: æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSV ({teacher_csv_path.name}) ã«å¯¾å¿œã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« ({metadata_filename}) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            
        return None, None, None

    def save_hmm_metadata(self, 
                          label_info: Tuple[List[str], Dict[str, int], Dict[int, str]], 
                          merged_csv_path: Path) -> Optional[Path]:
        """HMMå­¦ç¿’ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹"""
        sorted_labels, label_to_int, int_to_label = label_info
        
        if not sorted_labels or not label_to_int or not int_to_label:
            print("âŒ HMMãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«æƒ…å ±ãŒä¸å®Œå…¨ã§ã™ã€‚")
            return None

        n_components = len(sorted_labels)
        if n_components == 0:
            print("âŒ HMMãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«æ•°ãŒ0ã§ã™ã€‚")
            return None

        print(f"\n--- HMMç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ ---")
        try:
            metadata_content = {
                "n_components": int(n_components), 
                "phase_labels": [str(lbl) for lbl in sorted_labels], # å„ãƒ©ãƒ™ãƒ«ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                "label_to_int": {str(k): int(v) for k, v in label_to_int.items()}, # ã‚­ãƒ¼ã‚’æ–‡å­—åˆ—ã«ã€å€¤ã‚’intã«å¤‰æ›
                "int_to_label": {str(int(k)): str(v) for k, v in int_to_label.items()}, # ã‚­ãƒ¼ã‚’intã«ã—ã¦ã‹ã‚‰æ–‡å­—åˆ—ã«ã€å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                "source_merged_csv": merged_csv_path.name,
                "creation_timestamp": datetime.now().isoformat()
            }
            
            # ãƒãƒ¼ã‚¸æ¸ˆã¿CSVåã«åŸºã¥ã„ã¦HMMãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            # ä¾‹: merged_X_vs_Y_timestamp.csv -> hmm_metadata_merged_X_vs_Y_timestamp.json
            hmm_metadata_filename = f"hmm_metadata_{merged_csv_path.stem}.json"
            output_path = merged_csv_path.parent / hmm_metadata_filename # ãƒãƒ¼ã‚¸æ¸ˆã¿CSVã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_content, f, indent=4, ensure_ascii=False)
            
            print(f"âœ… HMMç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
            return output_path
        except Exception as e:
            print(f"âŒ HMMç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def save_merged_data(self, merged_df: pd.DataFrame, 
                         teacher_csv_name: str, pred_csv_name: str) -> Optional[Path]:
        """
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸDataFrameã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Args:
            merged_df (pd.DataFrame): ä¿å­˜ã™ã‚‹ãƒãƒ¼ã‚¸æ¸ˆã¿DataFrameã€‚
            teacher_csv_name (str): å…ƒã®æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆç”¨ï¼‰ã€‚
            pred_csv_name (str): å…ƒã®äºˆæ¸¬CSVãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆç”¨ï¼‰ã€‚

        Returns:
            Optional[Path]: ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã€‚
        """
        print(f"\n--- ãƒãƒ¼ã‚¸çµæœä¿å­˜ ---")
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã¨ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
            teacher_base = Path(teacher_csv_name).stem.replace("tennis_features_", "").replace("features_", "")
            pred_base = Path(pred_csv_name).stem.replace("lstm_predictions_", "").replace("predictions_", "")

            output_filename = f"merged_{teacher_base}_vs_{pred_base}_{timestamp}.csv"
            output_path = self.output_dir / output_filename
            
            merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"âœ… ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
            return output_path
        except Exception as e:
            print(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def run_merge_pipeline(self,
                           teacher_csv_path: Optional[Path] = None,
                           prediction_csv_path: Optional[Path] = None,
                           teacher_true_label_col: str = 'label', # feature_extractor.py ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›åˆ—å
                           prediction_pred_label_col: str = 'predicted_phase', # predict_lstm_model.py ã®æƒ³å®šå‡ºåŠ›åˆ—å
                           merge_keys: Optional[List[str]] = None
                           ):
        """
        ãƒãƒ¼ã‚¸å‡¦ç†ã®å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸æŠã‚’ä¿ƒã—ã¾ã™ã€‚
        """
        print("======= æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨LSTMäºˆæ¸¬ã®ãƒãƒ¼ã‚¸å‡¦ç†é–‹å§‹ =======")

        # 1. æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVã®é¸æŠã¨èª­ã¿è¾¼ã¿
        if teacher_csv_path is None:
            teacher_csv_path = self._select_file_dialog(
                title="æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ (feature_extractor.py å‡ºåŠ›)",
                directory=self.base_dir / "features", # feature_extractor.py ã®å‡ºåŠ›å…ˆæƒ³å®š
                pattern="tennis_features_*.csv"
            )
        teacher_df = self.load_csv(teacher_csv_path, "æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSV")
        if teacher_df is None:
            print("âŒ æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return

        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’å–å¾—è©¦è¡Œ
        hmm_labels_info = self._load_teacher_metadata_labels(teacher_csv_path)
        if hmm_labels_info[0] is None: # ãƒ©ãƒ™ãƒ«ãƒªã‚¹ãƒˆãŒNoneãªã‚‰å–å¾—å¤±æ•—
            print(f"â„¹ï¸ æ•™å¸«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’å–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVã® '{teacher_true_label_col}' åˆ—ã‹ã‚‰æ´¾ç”Ÿã•ã›ã¾ã™ã€‚")
            hmm_labels_info = self._derive_labels_from_df(teacher_df, teacher_true_label_col)

        if hmm_labels_info[0] is None:
            print(f"âŒ HMMç”¨ãƒ©ãƒ™ãƒ«æƒ…å ±ã®å–å¾—/æ´¾ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            # ã“ã®å¾Œã‚‚å‡¦ç†ã¯ç¶šè¡Œã™ã‚‹ãŒã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜ã•ã‚Œãªã„

        # 2. LSTMäºˆæ¸¬CSVã®é¸æŠã¨èª­ã¿è¾¼ã¿
        if prediction_csv_path is None:
            prediction_csv_path = self._select_file_dialog(
                title="LSTMäºˆæ¸¬CSVãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ (predict_lstm_model.py å‡ºåŠ›)",
                directory=self.base_dir / "predictions", # predict_lstm_model.py ã®å‡ºåŠ›å…ˆæƒ³å®š
                pattern="*.csv" # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¤‰æ›´
            )
        prediction_df = self.load_csv(prediction_csv_path, "LSTMäºˆæ¸¬CSV")
        if prediction_df is None:
            print("âŒ LSTMäºˆæ¸¬CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return

        # 3. ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸
        merged_df = self.merge_data(teacher_df, prediction_df, 
                                    teacher_true_label_col, prediction_pred_label_col,
                                    merge_keys)
        if merged_df is None or merged_df.empty:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ãŸã‹ã€çµæœãŒç©ºã§ã™ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return
            
        # 4. ãƒãƒ¼ã‚¸çµæœã®ä¿å­˜
        saved_merged_path = self.save_merged_data(merged_df, teacher_csv_path.name, prediction_csv_path.name)

        # 5. HMMç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ (ãƒ©ãƒ™ãƒ«æƒ…å ±ãŒå–å¾—ã§ãã¦ã„ã‚Œã°)
        if saved_merged_path and hmm_labels_info[0] is not None:
            self.save_hmm_metadata(hmm_labels_info, saved_merged_path)
        elif not hmm_labels_info[0]:
            print(f"â„¹ï¸ HMMç”¨ãƒ©ãƒ™ãƒ«æƒ…å ±ãŒãªã‹ã£ãŸãŸã‚ã€HMMãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")


        print("\nğŸ‰ ãƒãƒ¼ã‚¸å‡¦ç†å®Œäº†ï¼")


if __name__ == "__main__":
    # --- è¨­å®šé …ç›® ---
    # æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSV (feature_extractor.pyã®å‡ºåŠ›) ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    # ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã€ã¾ãŸã¯çµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®š
    BASE_TRAINING_DATA_DIR = Path("./training_data") 

    # ãƒãƒ¼ã‚¸æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå (BASE_TRAINING_DATA_DIR å†…)
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ "merged_predictions" ã§ã™ãŒã€ã“ã“ã§æ˜ç¤ºçš„ã«è¨­å®šã—ã¾ã™ã€‚
    MERGED_PREDICTIONS_DIR_NAME = "merged_predictions"

    # æ•™å¸«ãƒ‡ãƒ¼ã‚¿CSVå†…ã®ã€ŒçœŸã®ãƒ©ãƒ™ãƒ«ã€ãŒå«ã¾ã‚Œã‚‹åˆ—å
    # feature_extractor.py ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›ã§ã¯ 'label'
    TEACHER_TRUE_LABEL_COLUMN = 'label'

    # LSTMäºˆæ¸¬CSVå†…ã®ã€Œäºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã€ãŒå«ã¾ã‚Œã‚‹åˆ—å
    # predict_lstm_model.py ã®å‡ºåŠ›ã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
    PREDICTION_PREDICTED_LABEL_COLUMN = 'predicted_phase' 

    # ãƒãƒ¼ã‚¸ã«ä½¿ç”¨ã™ã‚‹ã‚­ãƒ¼åˆ—åã®ãƒªã‚¹ãƒˆ
    # ä¸¡æ–¹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å…±é€šã—ã¦å­˜åœ¨ã—ã€è¡Œã‚’ä¸€æ„ã«ç‰¹å®šã§ãã‚‹åˆ—ã‚’æŒ‡å®š
    # ä¾‹: ['video_name', 'frame_number'] ã‚„ ['video_name', 'original_frame_number']
    # `frame_number` ãŒ feature_extractor.py ã§å†å‰²ã‚Šå½“ã¦ã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å ´åˆã€
    # `original_frame_number` (å…ƒå‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·) ãŒã‚ã‚Œã°ãã¡ã‚‰ãŒå®‰å®šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
    # LSTMäºˆæ¸¬CSVã‚‚åŒã˜åŸºæº–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
    MERGE_KEYS = ['video_name', 'frame_number'] 
    # ----------------

    merger = MergeFeaturesAndPredictions(
        output_dir_name=MERGED_PREDICTIONS_DIR_NAME,
        base_dir=BASE_TRAINING_DATA_DIR
    )
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        # ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‘ã‚¹ã‚’æ¸¡ã—ã¾ã™ã€‚
        # teacher_file = BASE_TRAINING_DATA_DIR / "features" / "tennis_features_YYYYMMDD_HHMMSS.csv"
        # prediction_file = BASE_TRAINING_DATA_DIR / "predictions" / "lstm_predictions_some_model_YYYYMMDD_HHMMSS.csv"
        # merger.run_merge_pipeline(
        #     teacher_csv_path=teacher_file,
        #     prediction_csv_path=prediction_file,
        #     teacher_true_label_col=TEACHER_TRUE_LABEL_COLUMN,
        #     prediction_pred_label_col=PREDICTION_PREDICTED_LABEL_COLUMN,
        #     merge_keys=MERGE_KEYS
        # )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä¿ƒã™å ´åˆ
        merger.run_merge_pipeline(
            teacher_true_label_col=TEACHER_TRUE_LABEL_COLUMN,
            prediction_pred_label_col=PREDICTION_PREDICTED_LABEL_COLUMN,
            merge_keys=MERGE_KEYS
        )

    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

