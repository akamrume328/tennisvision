# predict_compatible.py

import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import joblib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Generator
import argparse
from model import TennisLSTMModel  # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ===== GPUè¨­å®š =====
def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"GPUæ¤œå‡º: {device_count}å°")
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        return torch.device('cuda')
    else:
        print("è­¦å‘Š: GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
        return torch.device('cpu')

DEVICE = setup_gpu_config()

# ===== æ¨è«–ã‚¯ãƒ©ã‚¹ =====
class TennisLSTMPredictor:
    def __init__(self, models_dir: str = "./training_data/lstm_models", 
                 input_features_dir: str = "./training_data/predict_features"):
        self.models_dir = Path(models_dir)
        self.input_features_dir = Path(input_features_dir)
        self.predictions_output_dir = Path("./training_data/predictions")
        self.predictions_output_dir.mkdir(parents=True, exist_ok=True)

        self.model: Optional[TennisLSTMModel] = None
        self.scaler = None # å‹ãƒ’ãƒ³ãƒˆã¯ joblib ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ä¾å­˜ã™ã‚‹ãŸã‚çœç•¥
        self.metadata: Optional[Dict] = None
        self.device = DEVICE
        
        self.phase_labels: List[str] = []
        self.feature_names: List[str] = []
        self.sequence_length: int = 30
        self.label_map_inv: Optional[Dict[int, str]] = None

    def select_model_files(self) -> Optional[Tuple[Path, Path, Path]]:
            print(f"\n=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
            
            # '**/...' ã‚’ä½¿ã£ã¦ã€ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å…¨ã¦ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«æ¤œç´¢
            all_model_files = sorted(
                list(self.models_dir.glob("**/tennis_pytorch*.pth")), 
                key=lambda p: p.stat().st_mtime, 
                reverse=True
            )

            # --- â–¼â–¼â–¼ ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–¼â–¼â–¼ ---
            print(f"ã€ãƒ‡ãƒãƒƒã‚°ã€‘ ç™ºè¦‹ã—ãŸ.pthãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°: {len(all_model_files)}")
            # --- â–²â–²â–² ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–²â–²â–² ---

            if not all_model_files:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (*.pth) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {self.models_dir} ãŠã‚ˆã³ãã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚")
                return None

            valid_sets = []
            for mf_path in all_model_files:
                # --- â–¼â–¼â–¼ ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–¼â–¼â–¼ ---
                print(f"\nã€ãƒ‡ãƒãƒƒã‚°ã€‘ ãƒã‚§ãƒƒã‚¯ä¸­ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {mf_path.name}")
                # --- â–²â–²â–² ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–²â–²â–² ---

                parent_dir = mf_path.parent
                
                if not mf_path.name.endswith("_model.pth"):
                    continue

                base_name = mf_path.name.removesuffix("_model.pth")
                scaler_path = parent_dir / f"{base_name}_scaler.pkl"
                meta_path = parent_dir / f"{base_name}_metadata.json"

                # --- â–¼â–¼â–¼ ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–¼â–¼â–¼ ---
                print(f"ã€ãƒ‡ãƒãƒƒã‚°ã€‘ æ¢ã—ã¦ã„ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {scaler_path.name}")
                print(f"ã€ãƒ‡ãƒãƒƒã‚°ã€‘ æ¢ã—ã¦ã„ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {meta_path.name}")
                print(f"ã€ãƒ‡ãƒãƒƒã‚°ã€‘ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å­˜åœ¨?: {scaler_path.exists()}, ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å­˜åœ¨?: {meta_path.exists()}")
                # --- â–²â–²â–² ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–²â–²â–² ---
                
                if scaler_path.exists() and meta_path.exists():
                    valid_sets.append((mf_path, scaler_path, meta_path))

            # --- â–¼â–¼â–¼ ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–¼â–¼â–¼ ---
            print(f"\nã€ãƒ‡ãƒãƒƒã‚°ã€‘ ç™ºè¦‹ã—ãŸæœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã®æ•°: {len(valid_sets)}")
            # --- â–²â–²â–² ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ—ãƒªãƒ³ãƒˆ â–²â–²â–² ---

            if not valid_sets:
                print(f"âŒ å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆï¼ˆscaler, metadataãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return None

            for i, (mf_path, _, _) in enumerate(valid_sets, 1):
                try:
                    display_path = mf_path.relative_to(self.models_dir)
                except ValueError:
                    display_path = mf_path
                
                print(f"  {i}. {display_path} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(mf_path.stat().st_mtime):%Y-%m-%d %H:%M})")
            
            try:
                choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(valid_sets)}): ").strip()
                choice_num = int(choice)
                return valid_sets[choice_num - 1] if 1 <= choice_num <= len(valid_sets) else None
            except (ValueError, IndexError):
                print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
                return None

    def load_model_and_metadata(self, model_path: Path, scaler_path: Path, metadata_path: Path) -> bool:
        print(f"\n--- ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---")
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f: self.metadata = json.load(f)
            print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {metadata_path.name}")

            self.scaler = joblib.load(scaler_path)
            print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿: {scaler_path.name}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å†…ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
            model_config = self.metadata['model_config']
            
            self.model = TennisLSTMModel(
                input_size=len(self.metadata['feature_names']), # ç‰¹å¾´é‡æ•°ã‹ã‚‰input_sizeã‚’å–å¾—
                num_classes=len(self.metadata['phase_labels']), # ãƒ©ãƒ™ãƒ«æ•°ã‹ã‚‰num_classesã‚’å–å¾—
                hidden_sizes=model_config['lstm_units'],
                dropout_rate=model_config.get('dropout_rate', 0.3),
                model_type=model_config.get('model_type', 'bidirectional'),
                use_batch_norm=model_config.get('batch_size', 64) > 1,
                enable_confidence_weighting=model_config.get('enable_confidence_weighting', False)
            )

            # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path.name}")

            # æ¨è«–ã«å¿…è¦ãªæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
            self.phase_labels = self.metadata['phase_labels']
            self.feature_names = self.metadata['feature_names']
            self.sequence_length = self.metadata.get('sequence_length', model_config.get('sequence_length', 30))
            self.label_map_inv = {i: label_name for i, label_name in enumerate(self.phase_labels)}
            
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«/ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback; traceback.print_exc()
            return False

    def select_input_feature_file(self) -> Optional[Path]:
        print(f"\n=== æ¨è«–ç”¨ ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        search_dirs = [self.input_features_dir, Path("./training_data")]
        feature_files = []
        for sdir in search_dirs:
            feature_files.extend(list(sdir.glob("tennis_inference_features_*.csv")))
        
        feature_files = sorted(list(set(feature_files)), key=lambda p: p.stat().st_mtime, reverse=True)

        if not feature_files:
            print(f"âŒ æ¨è«–ç”¨ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ« (*.csv) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        for i, f_path in enumerate(feature_files, 1):
            print(f"  {i}. {f_path.name} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(f_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(feature_files)}): ").strip()
            choice_num = int(choice)
            return feature_files[choice_num - 1] if 1 <= choice_num <= len(feature_files) else None
        except (ValueError, IndexError):
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def _generate_sequences_for_inference(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray]) -> Generator:
        num_frames = X_scaled.shape[0]
        if num_frames < self.sequence_length:
            return
        for i in range(num_frames - self.sequence_length + 1):
            seq_X = X_scaled[i : i + self.sequence_length]
            original_idx = i + self.sequence_length - 1
            seq_conf = confidence_scores[i : i + self.sequence_length] if confidence_scores is not None else None
            yield seq_X, original_idx, seq_conf

    def prepare_input_data(self, csv_path: Path) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame], Optional[np.ndarray]]:
        print(f"\n--- å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™: {csv_path.name} ---")
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ… CSVèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")

            missing_features = [f for f in self.feature_names if f not in df.columns]
            if missing_features:
                print(f"âŒ å¿…è¦ãªç‰¹å¾´é‡ãŒCSVã«ã‚ã‚Šã¾ã›ã‚“: {missing_features[:5]}...")
                return None, None, None
            
            X_df = df[self.feature_names].copy().fillna(0).replace([np.inf, -np.inf], 0)
            X_scaled = self.scaler.transform(X_df).astype(np.float32)

            confidence_scores = None
            if self.model and self.model.enable_confidence_weighting:
                if 'interpolation_ratio' in df.columns:
                    confidence_scores = (1.0 - df['interpolation_ratio'].fillna(0)).astype(np.float32).values
                else:
                    confidence_scores = np.ones(len(df), dtype=np.float32)
                    print("âš ï¸ 'interpolation_ratio'åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘ãªã—ã§å‡¦ç†ã—ã¾ã™ã€‚")
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ã€‚ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(df)}")
            return X_scaled, df, confidence_scores
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, None

    def predict(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray], batch_size: int = 256) -> Optional[Tuple]:
        if not self.model: return None
        print(f"\n--- æ¨è«–å®Ÿè¡Œ (ãƒãƒƒãƒå‡¦ç†) ---")
        self.model.eval()
        
        all_preds, all_probas, all_indices = [], [], []
        seq_generator = self._generate_sequences_for_inference(X_scaled, confidence_scores)
        
        batch_seq, batch_conf, batch_idx = [], [], []
        for seq_X, original_idx, seq_conf in seq_generator:
            batch_seq.append(seq_X)
            batch_idx.append(original_idx)
            if seq_conf is not None: batch_conf.append(seq_conf)

            if len(batch_seq) == batch_size:
                self._process_batch(batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices)
                batch_seq, batch_conf, batch_idx = [], [], []

        if batch_seq: # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
            self._process_batch(batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices)

        if not all_preds:
            print("âš ï¸  æ¨è«–å¯¾è±¡ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return np.array([]), np.array([]), []

        print(f"âœ… æ¨è«–å®Œäº†: {len(all_preds)}ä»¶")
        return np.array(all_preds), np.array(all_probas), all_indices

    def _process_batch(self, batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices):
        batch_X_tensor = torch.from_numpy(np.array(batch_seq, dtype=np.float32)).to(self.device)
        batch_conf_tensor = torch.from_numpy(np.array(batch_conf, dtype=np.float32)).to(self.device) if batch_conf else None
        
        with torch.no_grad():
            outputs = self.model(batch_X_tensor, batch_conf_tensor)
            probas = F.softmax(outputs, dim=1)
            _, preds = torch.max(probas, 1)
        
        all_preds.extend(preds.cpu().numpy())
        all_probas.extend(probas.cpu().numpy())
        all_indices.extend(batch_idx)

    def format_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, original_df: pd.DataFrame, original_indices: List[int]) -> pd.DataFrame:
        if not self.label_map_inv: return original_df
        
        results_df = original_df.copy()
        results_df['predicted_phase'] = ""
        results_df['prediction_confidence'] = np.nan

        # äºˆæ¸¬çµæœã‚’å¯¾å¿œã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        pred_series = pd.Series([self.label_map_inv.get(p, f"Unknown_{p}") for p in predictions], index=original_indices)
        conf_series = pd.Series(np.max(probabilities, axis=1), index=original_indices)
        
        results_df.loc[original_indices, 'predicted_phase'] = pred_series
        results_df.loc[original_indices, 'prediction_confidence'] = conf_series
        
        print("âœ… äºˆæ¸¬çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Œäº†")
        return results_df

    def save_predictions(self, predictions_df: pd.DataFrame, input_filename: str) -> Path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = Path(input_filename).stem
        output_filename = f"{base_name}_predictions_{timestamp}.csv"
        output_path = self.predictions_output_dir / output_filename
        
        predictions_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        return output_path

    def run_prediction_pipeline(self):
        """å¯¾è©±çš„ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        selected_files = self.select_model_files()
        if not selected_files: return
        
        if not self.load_model_and_metadata(*selected_files): return

        input_csv_path = self.select_input_feature_file()
        if not input_csv_path: return

        X_scaled, original_df, confidence_scores = self.prepare_input_data(input_csv_path)
        if X_scaled is None or original_df is None: return

        prediction_results = self.predict(X_scaled, confidence_scores)
        if prediction_results is None: return
        
        raw_preds, raw_probas, all_original_indices = prediction_results
        formatted_df = self.format_predictions(
            predictions=raw_preds, 
            probabilities=raw_probas, 
            original_df=original_df, 
            original_indices=all_original_indices
        )
        self.save_predictions(formatted_df, input_csv_path.name)

        print("\nğŸ‰ æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")

if __name__ == "__main__":
    print("=== ãƒ†ãƒ‹ã‚¹å‹•ç”»å±€é¢åˆ†é¡PyTorch LSTM æ¨è«–ãƒ„ãƒ¼ãƒ« ===")
    predictor = TennisLSTMPredictor()
    
    try:
        predictor.run_prediction_pipeline()
    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()