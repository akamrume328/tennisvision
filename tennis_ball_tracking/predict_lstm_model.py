import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import joblib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Union, Generator

# train_lstm_model.py ã‹ã‚‰å¿…è¦ãªå®šç¾©ã‚’ã‚³ãƒ”ãƒ¼ã¾ãŸã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# (ã“ã“ã§ã¯ç°¡ç•¥åŒ–ã®ãŸã‚ã€ä¸»è¦ãªã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã‚’ç›´æ¥è¨˜è¿°ã—ã¾ã™)

def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"ğŸš€ GPUæ¤œå‡º: {device_count}å°")
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        return torch.device('cuda')
    else:
        print("âš ï¸  GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
        return torch.device('cpu')

DEVICE = setup_gpu_config()

class TennisLSTMModel(nn.Module):
    """PyTorch LSTM ãƒ¢ãƒ‡ãƒ«ï¼ˆä¿¡é ¼åº¦é‡ã¿ä»˜ã‘å¯¾å¿œï¼‰"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, 
                 dropout_rate: float = 0.3, model_type: str = "bidirectional", 
                 use_batch_norm: bool = True, enable_confidence_weighting: bool = True):
        super(TennisLSTMModel, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.model_type = model_type
        self.use_batch_norm = use_batch_norm
        self.enable_confidence_weighting = enable_confidence_weighting
        
        if self.enable_confidence_weighting:
            self.confidence_attention = nn.Sequential(
                nn.Linear(input_size, hidden_sizes[0] // 4),
                nn.ReLU(),
                nn.Linear(hidden_sizes[0] // 4, 1),
                nn.Sigmoid()
            )
        
        self._build_lstm_layers()
        
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(self.lstm_output_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc_out = nn.Linear(64, num_classes)
        
        if self.use_batch_norm:
            self.bn1 = nn.BatchNorm1d(128)
            self.bn2 = nn.BatchNorm1d(64)
        else:
            self.ln1 = nn.LayerNorm(128)
            self.ln2 = nn.LayerNorm(64)
        
        self._init_weights()
    
    def _build_lstm_layers(self):
        if self.model_type == "simple":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[1], batch_first=True, dropout=self.dropout_rate)
            self.lstm_output_size = self.hidden_sizes[1]
        elif self.model_type == "bidirectional":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, 
                                bidirectional=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0] * 2, self.hidden_sizes[1], batch_first=True,
                                bidirectional=True, dropout=self.dropout_rate)
            self.lstm_output_size = self.hidden_sizes[1] * 2
        elif self.model_type == "stacked":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[1], batch_first=True, dropout=self.dropout_rate)
            self.lstm3 = nn.LSTM(self.hidden_sizes[1], 64, batch_first=True, dropout=self.dropout_rate)
            self.lstm_output_size = 64
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name: torch.nn.init.orthogonal_(param.data)
                    elif 'bias' in name: param.data.fill_(0)
            elif isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x, confidence_scores=None):
        if self.enable_confidence_weighting and confidence_scores is not None:
            # confidence_scores ã¯ (batch, seq_len) ã®å½¢çŠ¶ã‚’æœŸå¾…
            # attention_weights ã¯ (batch, seq_len, 1)
            attention_weights = self.confidence_attention(x) 
            # combined_weights ã¯ (batch, seq_len)
            combined_weights = attention_weights.squeeze(-1) * confidence_scores
            x = x * combined_weights.unsqueeze(-1) # (batch, seq_len, features)
        
        lstm_out = self._forward_lstm(x)
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        
        out = F.relu(self.fc1(lstm_out))
        out = self._apply_normalization(out, self.bn1 if self.use_batch_norm else self.ln1)
        out = self.dropout(out)
        
        out = F.relu(self.fc2(out))
        out = self._apply_normalization(out, self.bn2 if self.use_batch_norm else self.ln2)
        out = self.dropout(out)
        
        return self.fc_out(out)

    def _forward_lstm(self, x):
        if self.model_type == "simple":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
        elif self.model_type == "bidirectional":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
        elif self.model_type == "stacked":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm3(lstm_out)
        return lstm_out

    def _apply_normalization(self, x, norm_layer):
        if self.use_batch_norm and x.size(0) > 1:
            return norm_layer(x)
        elif not self.use_batch_norm:
            return norm_layer(x)
        return x

class TennisLSTMPredictor:
    def __init__(self, models_dir: str = "./training_data/lstm_models", 
                 input_features_dir: str = "./training_data/predict_features"):
        self.models_dir = Path(models_dir)
        self.input_features_dir = Path(input_features_dir)
        self.predictions_output_dir = Path("./training_data/predictions")
        self.predictions_output_dir.mkdir(parents=True, exist_ok=True)

        self.model: Optional[TennisLSTMModel] = None
        self.scaler: Optional[joblib.numpy_pickle.NumpyPickler] = None # sklearn.preprocessing.StandardScaler
        self.metadata: Optional[Dict] = None
        self.device = DEVICE
        
        self.phase_labels: List[str] = []
        self.feature_names: List[str] = []
        self.sequence_length: int = 30
        self.label_map_inv: Optional[Dict[int, str]] = None


    def select_model_files(self) -> Optional[Tuple[Path, Path, Path]]:
        print(f"\n=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        
        potential_model_dirs = [d for d in self.models_dir.iterdir() if d.is_dir()]
        if not potential_model_dirs:
            # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ã€models_dir ç›´ä¸‹ã‚’æ¤œç´¢ï¼ˆå¾“æ¥äº’æ›ï¼‰
            potential_model_dirs = [self.models_dir]

        print("åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆ:")
        valid_sets = []
        
        for model_subdir in potential_model_dirs:
            model_files = sorted(list(model_subdir.glob("tennis_pytorch_model_*.pth")))
            if not model_files:
                continue

            for mf_path in model_files:
                base_name = mf_path.name.replace("tennis_pytorch_model_", "").replace(".pth", "")
                scaler_path = model_subdir / f"tennis_pytorch_scaler_{base_name}.pkl"
                meta_path = model_subdir / f"tennis_pytorch_metadata_{base_name}.json"

                if scaler_path.exists() and meta_path.exists():
                    # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚‚è¡¨ç¤ºã«å«ã‚ã‚‹
                    display_name = f"{model_subdir.name}/{base_name}" if model_subdir != self.models_dir else base_name
                    valid_sets.append((mf_path, scaler_path, meta_path, display_name))
        
        if not valid_sets:
            print(f"âŒ å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆï¼ˆmodel, scaler, metadataï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {self.models_dir} ãŠã‚ˆã³ãã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚")
            return None

        # æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ (æ–°ã—ã„ã‚‚ã®ãŒä¸Šã«æ¥ã‚‹ã‚ˆã†ã«)
        valid_sets.sort(key=lambda x: x[0].stat().st_mtime, reverse=True)

        for i, (mf_path, _, _, display_name) in enumerate(valid_sets, 1):
            print(f"  {i}. {display_name} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(mf_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(valid_sets)}): ").strip()
            choice_num = int(choice)
            if 1 <= choice_num <= len(valid_sets):
                # display_name ã‚’é™¤ã„ãŸã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
                selected_set = valid_sets[choice_num - 1]
                return selected_set[0], selected_set[1], selected_set[2]
            else:
                print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
                return None
        except ValueError:
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def load_model_and_metadata(self, model_path: Path, scaler_path: Path, metadata_path: Path) -> bool:
        print(f"\n--- ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---")
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {metadata_path.name}")

            self.scaler = joblib.load(scaler_path)
            print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿: {scaler_path.name}")

            model_checkpoint = torch.load(model_path, map_location=self.device)
            model_config = model_checkpoint['model_config']
            
            self.model = TennisLSTMModel(
                input_size=model_config['input_size'],
                hidden_sizes=model_config['hidden_sizes'],
                num_classes=model_config['num_classes'],
                dropout_rate=model_config.get('dropout_rate', 0.3), # å¾Œæ–¹äº’æ›æ€§
                model_type=model_config.get('model_type', 'bidirectional'),
                use_batch_norm=model_config.get('use_batch_norm', True),
                enable_confidence_weighting=model_config.get('enable_confidence_weighting', False)
            )
            self.model.load_state_dict(model_checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path.name}")

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            self.phase_labels = self.metadata.get('phase_labels', [])
            self.feature_names = self.metadata.get('feature_names', [])
            self.sequence_length = self.metadata.get('sequence_length', 30)
            
            # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã®é€†ã‚’ä½œæˆ (äºˆæ¸¬çµæœã‚’ãƒ©ãƒ™ãƒ«åã«æˆ»ã™ãŸã‚)
            # metadata['label_map'] ã¯ {original_label_int: remapped_idx}
            # metadata['phase_labels'] ã¯ remapped_idx ã«å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«åãƒªã‚¹ãƒˆ
            # å¿…è¦ãªã®ã¯ remapped_idx -> phase_label_name
            if 'label_map' in self.metadata and self.phase_labels:
                 # phase_labels ã¯å­¦ç¿’æ™‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã ã£ãŸãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
                self.label_map_inv = {i: label_name for i, label_name in enumerate(self.phase_labels)}
            else: # å¤ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.label_map_inv = {i: label for i, label in enumerate(self.phase_labels)}

            if not self.feature_names:
                print("âš ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ (feature_names) ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return False
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«/ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def find_model_files_in_set_dir(self, model_set_dir: Path) -> Optional[Tuple[Path, Path, Path]]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’æ¤œç´¢ã™ã‚‹"""
        print(f"--- ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢: {model_set_dir} ---")
        model_files = sorted(list(model_set_dir.glob("tennis_pytorch_model_*.pth")), key=lambda p: p.stat().st_mtime, reverse=True)
        if not model_files:
            print(f"âŒ {model_set_dir} ã«ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (*.pth) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        latest_model_file = model_files[0]
        base_name = latest_model_file.name.replace("tennis_pytorch_model_", "").replace(".pth", "")
        
        scaler_path = model_set_dir / f"tennis_pytorch_scaler_{base_name}.pkl"
        meta_path = model_set_dir / f"tennis_pytorch_metadata_{base_name}.json"

        if scaler_path.exists() and meta_path.exists():
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {latest_model_file.name}")
            print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {scaler_path.name}")
            print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {meta_path.name}")
            return latest_model_file, scaler_path, meta_path
        else:
            print(f"âŒ {model_set_dir} ã«å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆ (scaler or metadata) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            if not scaler_path.exists(): print(f"   - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {scaler_path}")
            if not meta_path.exists(): print(f"   - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {meta_path}")
            return None

    def select_input_feature_file(self) -> Optional[Path]:
        print(f"\n=== æ¨è«–ç”¨ ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        feature_files = sorted(list(self.input_features_dir.glob("tennis_inference_features_*.csv")))
        
        if not feature_files:
            # training_data ç›´ä¸‹ã‚‚æ¤œç´¢ (feature_extractor_predict.py ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¿å­˜å…ˆå¤‰æ›´å‰ã®äº’æ›æ€§)
            feature_files.extend(sorted(list(Path("./training_data").glob("tennis_inference_features_*.csv"))))
            if not feature_files:
                 print(f"âŒ æ¨è«–ç”¨ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ« (*.csv) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {self.input_features_dir} or ./training_data")
                 return None

        for i, f_path in enumerate(feature_files, 1):
            print(f"  {i}. {f_path.name} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(f_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(feature_files)}): ").strip()
            choice_num = int(choice)
            if 1 <= choice_num <= len(feature_files):
                return feature_files[choice_num - 1]
            else:
                print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚")
                return None
        except ValueError:
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def _generate_sequences_for_inference(self, X_scaled: np.ndarray, 
                                          confidence_scores_full: Optional[np.ndarray] = None
                                          ) -> Generator[Tuple[np.ndarray, int, Optional[np.ndarray]], None, None]:
        """
        æ¨è«–ç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã€å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ä¿¡é ¼åº¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
        """
        num_frames = X_scaled.shape[0]
        if num_frames < self.sequence_length:
            print(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿é•· ({num_frames}) ãŒã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· ({self.sequence_length}) ã‚ˆã‚ŠçŸ­ã„ãŸã‚ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚")
            return

        for i in range(num_frames - self.sequence_length + 1):
            seq_X = X_scaled[i : i + self.sequence_length]
            original_idx = i + self.sequence_length - 1 # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

            seq_conf = None
            if confidence_scores_full is not None:
                seq_conf = confidence_scores_full[i : i + self.sequence_length]
            
            yield seq_X, original_idx, seq_conf

    def prepare_input_data(self, csv_path: Path) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame], Optional[np.ndarray]]:
        print(f"\n--- å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™: {csv_path.name} ---")
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ… CSVèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, {len(df.columns)}åˆ—")

            # ç‰¹å¾´é‡é¸æŠ
            missing_features = [f for f in self.feature_names if f not in df.columns]
            if missing_features:
                print(f"âŒ å¿…è¦ãªç‰¹å¾´é‡ãŒCSVã«ã‚ã‚Šã¾ã›ã‚“: {missing_features}")
                return None, None, None
            
            X_df = df[self.feature_names].copy()

            # æ¬ æå€¤å‡¦ç†
            X_df = X_df.fillna(0)
            X_df = X_df.replace([np.inf, -np.inf], 0)

            X_scaled = self.scaler.transform(X_df)
            X_scaled = X_scaled.astype(np.float32) # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›

            confidence_scores_full = None
            if self.model and self.model.enable_confidence_weighting:
                if 'interpolated' in df.columns:
                    base_confidence = 1.0 - df['interpolated'].astype(np.float32) * 0.3
                    confidence_scores_full = base_confidence.astype(np.float32).values
                elif 'data_quality' in df.columns:
                     confidence_scores_full = df['data_quality'].fillna(1.0).astype(np.float32).values
                else:
                    print("âš ï¸  ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘ãŒæœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€ä¿¡é ¼åº¦é–¢é€£åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¿¡é ¼åº¦1.0ã§å‡¦ç†ã—ã¾ã™ã€‚")
                    confidence_scores_full = np.ones(len(df), dtype=np.float32)
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ã€‚ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(df)}")
            return X_scaled, df, confidence_scores_full

        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None

    def predict(self, X_scaled: np.ndarray,
                original_df: pd.DataFrame,
                confidence_scores_full: Optional[np.ndarray] = None,
                batch_size: int = 256  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´å¯èƒ½ã«ã™ã‚‹
                ) -> Optional[Tuple[np.ndarray, np.ndarray, List[int]]]:
        if self.model is None:
            print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return None
        
        print(f"\n--- æ¨è«–å®Ÿè¡Œ (ãƒãƒƒãƒå‡¦ç†) ---")
        self.model.eval()
        
        all_preds_list = []
        all_probas_list = []
        all_original_indices_list = []

        current_batch_sequences = []
        current_batch_confidences = [] if self.model.enable_confidence_weighting and confidence_scores_full is not None else None
        current_batch_indices = []

        num_sequences_processed = 0
        total_sequences_to_generate = max(0, X_scaled.shape[0] - self.sequence_length + 1)

        seq_generator = self._generate_sequences_for_inference(X_scaled, confidence_scores_full)

        for seq_X, original_idx, seq_conf in seq_generator:
            current_batch_sequences.append(seq_X)
            current_batch_indices.append(original_idx)
            if current_batch_confidences is not None and seq_conf is not None:
                current_batch_confidences.append(seq_conf)

            if len(current_batch_sequences) == batch_size:
                batch_X_np = np.array(current_batch_sequences, dtype=np.float32)
                batch_X_tensor = torch.from_numpy(batch_X_np).to(self.device)
                
                batch_conf_tensor = None
                if current_batch_confidences:
                    batch_conf_np = np.array(current_batch_confidences, dtype=np.float32)
                    batch_conf_tensor = torch.from_numpy(batch_conf_np).to(self.device)

                with torch.no_grad():
                    outputs = self.model(batch_X_tensor, batch_conf_tensor)
                    probas = F.softmax(outputs, dim=1)
                    _, preds = torch.max(probas, 1)
                
                all_preds_list.extend(preds.cpu().numpy())
                all_probas_list.extend(probas.cpu().numpy())
                all_original_indices_list.extend(current_batch_indices)
                num_sequences_processed += len(current_batch_sequences)

                current_batch_sequences = []
                current_batch_indices = []
                if current_batch_confidences is not None:
                    current_batch_confidences = []
                
                if num_sequences_processed % (batch_size * 10) == 0: # é€²æ—è¡¨ç¤º
                     print(f"   å‡¦ç†æ¸ˆã¿ã‚·ãƒ¼ã‚±ãƒ³ã‚¹: {num_sequences_processed} / {total_sequences_to_generate}")
        
        # æ®‹ã‚Šã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†
        if current_batch_sequences:
            batch_X_np = np.array(current_batch_sequences, dtype=np.float32)
            batch_X_tensor = torch.from_numpy(batch_X_np).to(self.device)
            
            batch_conf_tensor = None
            if current_batch_confidences:
                batch_conf_np = np.array(current_batch_confidences, dtype=np.float32)
                batch_conf_tensor = torch.from_numpy(batch_conf_np).to(self.device)

            with torch.no_grad():
                outputs = self.model(batch_X_tensor, batch_conf_tensor)
                probas = F.softmax(outputs, dim=1)
                _, preds = torch.max(probas, 1)

            all_preds_list.extend(preds.cpu().numpy())
            all_probas_list.extend(probas.cpu().numpy())
            all_original_indices_list.extend(current_batch_indices)
            num_sequences_processed += len(current_batch_sequences)

        if not all_preds_list:
            print("âš ï¸  æ¨è«–å¯¾è±¡ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return np.array([]), np.array([]), []

        print(f"âœ… æ¨è«–å®Œäº†: {num_sequences_processed}ä»¶ (å…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹)")
        return np.array(all_preds_list), np.array(all_probas_list), all_original_indices_list

    def format_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, 
                           original_df: pd.DataFrame, original_indices: List[int]) -> pd.DataFrame:
        if self.label_map_inv is None:
            print("âŒ ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒæœªå®šç¾©ã§ã™ã€‚")
            return original_df
        
        # çµæœã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã®æ–°ã—ã„åˆ—ã‚’æº–å‚™
        num_frames = len(original_df)
        predicted_label_names = [""] * num_frames
        max_probabilities = np.zeros(num_frames)
        
        # å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚‚ä¿å­˜ã™ã‚‹å ´åˆ
        # proba_columns = {f"proba_{self.label_map_inv.get(i, f'class_{i}')}": np.zeros(num_frames) for i in range(probabilities.shape[1])}

        for i, pred_idx in enumerate(predictions):
            frame_idx = original_indices[i] # ã“ã®äºˆæ¸¬ãŒå¯¾å¿œã™ã‚‹å…ƒã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            if 0 <= frame_idx < num_frames:
                predicted_label_names[frame_idx] = self.label_map_inv.get(pred_idx, f"Unknown_{pred_idx}")
                max_probabilities[frame_idx] = probabilities[i, pred_idx]
                # for class_idx in range(probabilities.shape[1]):
                #    proba_columns[f"proba_{self.label_map_inv.get(class_idx, f'class_{class_idx}')}"][frame_idx] = probabilities[i, class_idx]


        results_df = original_df.copy()
        results_df['predicted_phase'] = predicted_label_names
        results_df['prediction_confidence'] = max_probabilities
        
        # for col_name, prob_values in proba_columns.items():
        #    results_df[col_name] = prob_values
            
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€åˆã®æ•°ãƒ•ãƒ¬ãƒ¼ãƒ ã¯äºˆæ¸¬ãŒãªã„ãŸã‚ã€ãã‚Œã‚‰ã‚’ã©ã†æ‰±ã†ã‹
        # ã“ã“ã§ã¯ç©ºæ–‡å­—ã¾ãŸã¯NaNã®ã¾ã¾
        results_df.loc[results_df['predicted_phase'] == "", 'prediction_confidence'] = np.nan

        print("âœ… äºˆæ¸¬çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Œäº†")
        return results_df

    def save_predictions(self, predictions_df: pd.DataFrame, input_filename: str) -> Path:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = Path(input_filename).stem
        output_filename = f"{base_name}_predictions_{timestamp}.csv"
        output_path = self.predictions_output_dir / output_filename
        
        predictions_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        return output_path

    def run_prediction_pipeline(self):
        # 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        selected_files = self.select_model_files()
        if not selected_files: return
        model_p, scaler_p, meta_p = selected_files

        # 2. ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_model_and_metadata(model_p, scaler_p, meta_p): return

        # 3. å…¥åŠ›ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        input_csv_path = self.select_input_feature_file()
        if not input_csv_path: return

        # 4. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X_scaled, original_df, confidence_scores_full = self.prepare_input_data(input_csv_path)
        if X_scaled is None or original_df is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return

        # 5. æ¨è«–å®Ÿè¡Œ
        prediction_results = self.predict(X_scaled, original_df, confidence_scores_full)
        if prediction_results is None:
            print("âŒ æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
            return
        raw_preds, raw_probas, all_original_indices = prediction_results
        
        # 6. çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_df = self.format_predictions(raw_preds, raw_probas, original_df, all_original_indices)

        # 7. çµæœä¿å­˜
        self.save_predictions(formatted_df, input_csv_path.name)

        print("\nğŸ‰ æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")

    def run_prediction_for_file(self, model_set_path: Path, feature_csv_path: Path) -> Optional[Path]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã¨ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã€çµæœã®CSVãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å‘¼ã³å‡ºã—ç”¨ã€‚
        """
        print(f"\n=== éå¯¾è©±çš„æ¨è«–é–‹å§‹ ===")
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‘ã‚¹: {model_set_path}")
        print(f"ç‰¹å¾´é‡CSVãƒ‘ã‚¹: {feature_csv_path}")

        # 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®šã¨èª­ã¿è¾¼ã¿
        model_files_tuple = self.find_model_files_in_set_dir(model_set_path)
        if not model_files_tuple:
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ {model_set_path} ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        model_p, scaler_p, meta_p = model_files_tuple

        if not self.load_model_and_metadata(model_p, scaler_p, meta_p):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {model_p.name}")
            return None

        # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X_scaled, original_df, confidence_scores_full = self.prepare_input_data(feature_csv_path)
        if X_scaled is None or original_df is None:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {feature_csv_path.name}")
            return None

        # 3. æ¨è«–å®Ÿè¡Œ
        prediction_results = self.predict(X_scaled, original_df, confidence_scores_full)
        if prediction_results is None:
            print("âŒ æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return None
        raw_preds, raw_probas, all_original_indices = prediction_results
        
        # 4. çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_df = self.format_predictions(raw_preds, raw_probas, original_df, all_original_indices)

        # 5. çµæœä¿å­˜
        output_csv_path = self.save_predictions(formatted_df, feature_csv_path.name)
        
        print(f"\nğŸ‰ éå¯¾è©±çš„æ¨è«–å®Œäº†ï¼çµæœ: {output_csv_path}")
        return output_csv_path

if __name__ == "__main__":
    print("=== ãƒ†ãƒ‹ã‚¹å‹•ç”»å±€é¢åˆ†é¡PyTorch LSTM æ¨è«–ãƒ„ãƒ¼ãƒ« ===")
    predictor = TennisLSTMPredictor()
    
    try:
        predictor.run_prediction_pipeline()
    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

