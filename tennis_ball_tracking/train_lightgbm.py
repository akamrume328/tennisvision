import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
import yaml
import joblib
import json
from pathlib import Path
from datetime import datetime
import argparse
import warnings
import shap

from sklearn.model_selection import GroupKFold, GroupShuffleSplit
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

warnings.filterwarnings('ignore', category=UserWarning)

def load_config(path: str) -> dict:
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {path}")
        return config
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise

def load_dataset(data_dir: Path, csv_path: str = None) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if not csv_path:
        all_files = list(data_dir.glob("features/*.csv")) + list(data_dir.glob("*.csv"))
        if not all_files:
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_dir}")
        csv_path = str(max(all_files, key=lambda p: p.stat().st_mtime))

    dataset_path = Path(csv_path)
    if not dataset_path.exists():
        raise FileNotFoundError(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dataset_path}")

    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {dataset_path.name}")
    df = pd.read_csv(dataset_path)
    print(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ")
    return df, csv_path

def main():
    """ãƒ¡ã‚¤ãƒ³ã®å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆäº¤å·®æ¤œè¨¼å¯¾å¿œï¼‰"""
    parser = argparse.ArgumentParser(description="LightGBMã«ã‚ˆã‚‹ãƒ†ãƒ‹ã‚¹å±€é¢åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ„ãƒ¼ãƒ«ï¼ˆäº¤å·®æ¤œè¨¼å¯¾å¿œï¼‰")
    parser.add_argument('--config', type=str, default='config_lightgbm.yaml', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--csv_path', type=str, default=None, help='(ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    args = parser.parse_args()

    # 1. è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    config = load_config(args.config)
    df, loaded_csv_path = load_dataset(Path(config['training_data_dir']), args.csv_path)
    final_model = None  # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®å¤‰æ•°

    # 2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
    df.dropna(subset=['label'], inplace=True)
    df['label'] = df['label'].astype(int)

    exclude_cols = ['label', 'video_name', 'frame_number', 'original_frame_number', 'timestamp', 'interpolated']
    feature_columns = [c for c in df.columns if c not in exclude_cols]
    X = df[feature_columns]
    y = df['label']
    groups = df['video_name']

    feature_list_path = config.get('feature_list_path')
    if feature_list_path and Path(feature_list_path).exists():
        print(f"\nç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {feature_list_path}")
        with open(feature_list_path, 'r', encoding='utf-8') as f:
            top_features = [line.strip() for line in f]
        top_features_existing = [f for f in top_features if f in X.columns]
        print(f"ä¸Šä½{len(top_features_existing)}å€‹ã®ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚")
        X = X[top_features_existing]
        feature_columns = top_features_existing
    else:
        print("\nå…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚")

    phase_labels_map = {
        0: "point_interval", 1: "rally", 2: "serve_front_deuce", 3: "serve_front_ad",
        4: "serve_back_deuce", 5: "serve_back_ad", 6: "changeover"
    }
    active_labels_indices = sorted(y.unique())
    active_phase_labels = [phase_labels_map[i] for i in active_labels_indices]
    
    # 3. äº¤å·®æ¤œè¨¼ã®æº–å‚™
    n_splits = config.get('n_splits', 5)
    gkf = GroupKFold(n_splits=n_splits)

    df['oof_prediction_id'] = -1  # OOFäºˆæ¸¬ç”¨ã®åˆ—ã‚’è¿½åŠ 
    
    all_preds = []
    all_true = []
    fold_scores = {'accuracy': [], 'f1_score': []}
    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å…¨Foldåˆ†ä¿å­˜ã™ã‚‹ãŸã‚ã®DataFrame
    feature_importances = pd.DataFrame(index=feature_columns)

    print(f"\n===== {n_splits}åˆ†å‰²ã‚°ãƒ«ãƒ¼ãƒ—äº¤å·®æ¤œè¨¼ã‚’é–‹å§‹ã—ã¾ã™ =====")

    # 4. äº¤å·®æ¤œè¨¼ãƒ«ãƒ¼ãƒ—
    for fold, (train_val_idx, test_idx) in enumerate(gkf.split(X, y, groups)):
        print(f"\n--- FOLD {fold + 1}/{n_splits} ---")
        
        # --- ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ---
        # Foldã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã¾ãšç¢ºä¿
        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]
        
        # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¢ºä¿
        X_train_val, y_train_val = X.iloc[train_val_idx], y.iloc[train_val_idx]
        groups_train_val = groups.iloc[train_val_idx]

        # â˜…â˜…â˜… æ—©æœŸåœæ­¢ã®ãŸã‚ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ†å‰²ï¼ˆæƒ…å ±ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰ â˜…â˜…â˜…
        gss_val = GroupShuffleSplit(n_splits=1, test_size=config['validation_size'], random_state=config['random_state'])
        train_idx, val_idx = next(gss_val.split(X_train_val, y_train_val, groups_train_val))
        
        X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]
        X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]

        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X_train)}ä»¶, æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}ä»¶, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(X_test)}ä»¶")

        # --- ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ---
        model = lgb.LGBMClassifier(**config['lgbm_params'])
        callbacks = [
            lgb.early_stopping(stopping_rounds=config['early_stopping_rounds'], verbose=True),
            lgb.log_evaluation(period=100)
        ]
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)], # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã¯ãªãã€æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã§ç›£è¦–
            eval_metric=config['lgbm_params'].get('metric', 'multi_logloss'),
            callbacks=callbacks
        )
        
        # --- è©•ä¾¡ã¨çµæœã®é›†è¨ˆ ---
        y_pred = model.predict(X_test)
        df.loc[test_idx, 'oof_prediction_id'] = y_pred  # OOFäºˆæ¸¬çµæœã‚’ä¿å­˜
        all_preds.extend(y_pred)
        all_true.extend(y_test)
        
        fold_acc = accuracy_score(y_test, y_pred)
        fold_f1 = f1_score(y_test, y_pred, average='weighted')
        fold_scores['accuracy'].append(fold_acc)
        fold_scores['f1_score'].append(fold_f1)
        print(f"FOLD {fold + 1} çµæœ: Accuracy={fold_acc:.4f}, F1-score={fold_f1:.4f}")

        # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ä¿å­˜
        feature_importances[f'fold_{fold+1}'] = model.feature_importances_

    # 5. äº¤å·®æ¤œè¨¼ã®æœ€çµ‚çµæœè©•ä¾¡
    print("\n===== äº¤å·®æ¤œè¨¼ æœ€çµ‚çµæœ =====")
    avg_acc = np.mean(fold_scores['accuracy'])
    std_acc = np.std(fold_scores['accuracy'])
    avg_f1 = np.mean(fold_scores['f1_score'])
    std_f1 = np.std(fold_scores['f1_score'])
    
    print(f"å¹³å‡ç²¾åº¦: {avg_acc:.4f} (+/- {std_acc:.4f})")
    print(f"å¹³å‡F1ã‚¹ã‚³ã‚¢: {avg_f1:.4f} (+/- {std_f1:.4f})")
    
    print("\nç·åˆåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ (å…¨Fold):")
    report = classification_report(all_true, all_preds, target_names=active_phase_labels, zero_division=0)
    print(report)

    # 6. çµæœã®ä¿å­˜
    output_dir = Path(config['output_dir'])
    output_dir.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(all_true, all_preds, labels=active_labels_indices)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=active_phase_labels, yticklabels=active_phase_labels)
    plt.title(f'Overall Confusion Matrix (Avg Acc: {avg_acc:.4f})')
    plt.xlabel('Predicted Phase'); plt.ylabel('True Phase'); plt.tight_layout()
    cm_path = output_dir / f'cv_confusion_matrix_{timestamp}.png'
    plt.savefig(cm_path)
    print(f"\nç·åˆæ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {cm_path}")

    # ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå¹³å‡ï¼‰
    feature_importances['mean'] = feature_importances.mean(axis=1)
    feature_importances.sort_values('mean', ascending=False, inplace=True)
    
    plt.figure(figsize=(12, 18))
    sns.barplot(x='mean', y=feature_importances.index[:50], data=feature_importances.head(50))
    plt.title('Average Feature Importance over Folds (Top 50)')
    plt.tight_layout()
    fi_path = output_dir / f'cv_feature_importance_{timestamp}.png'
    plt.savefig(fi_path)
    print(f"å¹³å‡ç‰¹å¾´é‡é‡è¦åº¦ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fi_path}")
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    report_path = output_dir / f'cv_classification_report_{timestamp}.txt'
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("===== äº¤å·®æ¤œè¨¼ æœ€çµ‚çµæœ =====\n\n")
            f.write(f"å¹³å‡ç²¾åº¦: {avg_acc:.4f} (+/- {std_acc:.4f})\n")
            f.write(f"å¹³å‡F1ã‚¹ã‚³ã‚¢: {avg_f1:.4f} (+/- {std_f1:.4f})\n\n")
            f.write("ç·åˆåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ (å…¨Fold):\n")
            f.write(report) # classification_report ã®çµæœã‚’æ›¸ãè¾¼ã‚€
        print(f"åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {report_path}")
    except Exception as e:
        print(f"âŒ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # 7. (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ãƒ»ä¿å­˜
    if config.get('retrain_final_model', False):
        print("\nå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¾ã™...")
        final_model = lgb.LGBMClassifier(**config['lgbm_params'])
        # ã“ã“ã§ã¯æ—©æœŸåœæ­¢ãªã—ã§ã€äº¤å·®æ¤œè¨¼ã§å¾—ã‚‰ã‚ŒãŸæœ€é©ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ãªã©ã‚’å‚è€ƒã«å­¦ç¿’ã™ã‚‹
        # best_iterationã¯å„Foldã§ç•°ãªã‚‹ãŸã‚ã€ã“ã“ã§ã¯å˜ç´”ã«å…¨ãƒ‡ãƒ¼ã‚¿ã§fitã•ã›ã‚‹
        final_model.fit(X, y)
        print("æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        
        model_path = output_dir / f'lgbm_final_model_{timestamp}.pkl'
        joblib.dump(final_model, model_path)
        print(f"æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    metadata = {
        'timestamp': timestamp,
        'model_type': 'LightGBM_CV',
        'dataset_path': loaded_csv_path,
        'config': config,
        'phase_labels': active_phase_labels,
        'cv_evaluation': {
            'avg_accuracy': avg_acc, 'std_accuracy': std_acc,
            'avg_f1_score': avg_f1, 'std_f1_score': std_f1,
            'classification_report': classification_report(all_true, all_preds, target_names=active_phase_labels, output_dict=True, zero_division=0)
        },
        'feature_names': feature_columns
    }
    metadata_path = output_dir / f'metadata_cv_{timestamp}.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {metadata_path}")

    # 8. SHAPã«ã‚ˆã‚‹å±€é¢ã”ã¨ã®ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ
    if final_model is not None:
        print("\n===== SHAPã«ã‚ˆã‚‹å±€é¢ã”ã¨ã®ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã‚’é–‹å§‹ã—ã¾ã™ =====")
        
        sample_size = min(config.get('shap_sample_size', 5000), len(X))
        X_sample = X.sample(n=sample_size, random_state=config.get('random_state', 42))
        
        print(f"{sample_size}ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§SHAPå€¤ã‚’è¨ˆç®—ã—ã¾ã™...")
        
        explainer = shap.TreeExplainer(final_model)
        shap_values = explainer.shap_values(X_sample)
        
        print("SHAPå€¤ã®è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚è¦ç´„ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¾ã™...")
        
        # --- SHAPãƒ—ãƒ­ãƒƒãƒˆã®ä¿å­˜ï¼ˆå¤‰æ›´ãªã—ï¼‰ ---
        plt.figure()
        shap.summary_plot(
            shap_values,
            X_sample,
            plot_type="bar",
            class_names=active_phase_labels,
            max_display=50,
            show=False
        )
        fig = plt.gcf()
        fig.set_size_inches(15, 25)
        plt.title('SHAP Feature Importance by Phase', fontsize=16)
        plt.tight_layout()
        
        shap_plot_path = output_dir / f'shap_summary_plot_{timestamp}.png'
        plt.savefig(shap_plot_path)
        plt.close()
        
        print(f"âœ… SHAPã®è¦ç´„ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {shap_plot_path}")

        # --- â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ãŒä»Šå›ã®ä¿®æ­£ç®‡æ‰€ã§ã™ â–¼â–¼â–¼ ---
        
        print("SHAPã®æ•°å€¤ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™...")
        try:
            # 1. ç©ºã®DataFrameã‚’ã€æ­£ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆç‰¹å¾´é‡åï¼‰ã§ä½œæˆ
            df_shap_values = pd.DataFrame(index=X_sample.columns)

            # 2. ãƒ«ãƒ¼ãƒ—ã§ã‚¯ãƒ©ã‚¹ã”ã¨ã«è¨ˆç®—ã—ã€åˆ—ã¨ã—ã¦è¿½åŠ ã—ã¦ã„ã
            for i, class_name in enumerate(active_phase_labels):
                # ã‚¯ãƒ©ã‚¹iã«å¯¾ã™ã‚‹å…¨ã‚µãƒ³ãƒ—ãƒ«ã®SHAPå€¤ã®çµ¶å¯¾å€¤ã‚’ã¨ã‚Šã€ç‰¹å¾´é‡ã”ã¨ã«å¹³å‡ã‚’è¨ˆç®—
                # ã“ã®çµæœã¯ (ç‰¹å¾´é‡æ•°,) ã®å½¢çŠ¶ã®é…åˆ—ã«ãªã‚‹
                mean_abs_shap = np.abs(shap_values[i]).mean(axis=0)
                df_shap_values[class_name] = mean_abs_shap

            # 3. å…¨ä½“çš„ãªé‡è¦åº¦ã‚’è¨ˆç®—ã—ã¦ã‚½ãƒ¼ãƒˆ
            df_shap_values['global_importance'] = df_shap_values.sum(axis=1)
            df_shap_values.sort_values('global_importance', ascending=False, inplace=True)
            
            # 4. CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            shap_csv_path = output_dir / f'shap_values_{timestamp}.csv'
            df_shap_values.to_csv(shap_csv_path, encoding='utf-8-sig')
            
            print(f"âœ… SHAPã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {shap_csv_path}")

        except Exception as e:
            print(f"âŒ SHAPã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc() # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º

    # OOFäºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜
    if config.get('save_oof_predictions', True):
        print("\nOOFäºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã™...")
        df['predicted_phase'] = df['oof_prediction_id'].map(phase_labels_map)
        oof_output_df = df[[
            'video_name', 
            'frame_number', 
            'label', # æ­£è§£ãƒ©ãƒ™ãƒ«
            'predicted_phase' # OOFäºˆæ¸¬ãƒ©ãƒ™ãƒ«
        ]].copy()
        
        # oof_prediction_id ãŒ -1 ã®ã¾ã¾ã®è¡Œï¼ˆä½•ã‚‰ã‹ã®ç†ç”±ã§äºˆæ¸¬ã•ã‚Œãªã‹ã£ãŸè¡Œï¼‰ã‚’é™¤å¤–
        oof_output_df = oof_output_df[df['oof_prediction_id'] != -1].copy()

        oof_csv_path = output_dir / f'lgbm_oof_predictions_{timestamp}.csv'
        oof_output_df.to_csv(oof_csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… HMMå­¦ç¿’ç”¨ã®OOFäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {oof_csv_path}")

    print("\nğŸ‰ LightGBMã®äº¤å·®æ¤œè¨¼ã¨OOFäºˆæ¸¬ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


if __name__ == '__main__':
    main()