import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import joblib

# PyTorch with GPU configuration
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import GradScaler, autocast

# GPUè¨­å®šã¨CUDAæœ€é©åŒ–
def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    try:
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            print(f"ğŸš€ GPUæ¤œå‡º: {device_count}å°")
            
            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
            
            # CUDAã®è©³ç´°æƒ…å ±
            cuda_version = torch.version.cuda
            cudnn_version = torch.backends.cudnn.version()
            print(f"âœ… CUDA version: {cuda_version}")
            print(f"âœ… cuDNN version: {cudnn_version}")
            
            # æ··åˆç²¾åº¦å­¦ç¿’ã®ç¢ºèª
            if torch.cuda.is_bf16_supported():
                print("âœ… BF16æ··åˆç²¾åº¦å­¦ç¿’å¯¾å¿œ")
            else:
                print("âœ… FP16æ··åˆç²¾åº¦å­¦ç¿’å¯¾å¿œ")
            
            # CuDNNæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.enabled = True
            print("âœ… CuDNNæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
            
            device = torch.device('cuda')
            return device
        else:
            print("âš ï¸  GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
            device = torch.device('cpu')
            return device
            
    except Exception as e:
        print(f"âš ï¸  GPUè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        print("CPUã§å®Ÿè¡Œã—ã¾ã™")
        device = torch.device('cpu')
        return device

# GPUè¨­å®šã‚’æœ€åˆã«å®Ÿè¡Œ
DEVICE = setup_gpu_config()
GPU_AVAILABLE = DEVICE.type == 'cuda'

plt.style.use('default')
sns.set_palette("husl")

class TennisLSTMModel(nn.Module):
    """PyTorch LSTM ãƒ¢ãƒ‡ãƒ«"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, 
                 dropout_rate: float = 0.3, model_type: str = "bidirectional"):
        super(TennisLSTMModel, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.model_type = model_type
        
        # LSTMå±¤ã‚’æ§‹ç¯‰
        if model_type == "simple":
            self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, dropout=dropout_rate)
            self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True, dropout=dropout_rate)
            lstm_output_size = hidden_sizes[1]
            
        elif model_type == "bidirectional":
            self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, 
                                bidirectional=True, dropout=dropout_rate)
            self.lstm2 = nn.LSTM(hidden_sizes[0] * 2, hidden_sizes[1], batch_first=True, 
                                bidirectional=True, dropout=dropout_rate)
            lstm_output_size = hidden_sizes[1] * 2
            
        elif model_type == "stacked":
            self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, dropout=dropout_rate)
            self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True, dropout=dropout_rate)
            self.lstm3 = nn.LSTM(hidden_sizes[1], 64, batch_first=True, dropout=dropout_rate)
            lstm_output_size = 64
        
        # Dropoutå±¤
        self.dropout = nn.Dropout(dropout_rate)
        
        # Denseå±¤
        self.fc1 = nn.Linear(lstm_output_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc_out = nn.Linear(64, num_classes)
        
        # Batch Normalization
        self.bn1 = nn.BatchNorm1d(128)
        self.bn2 = nn.BatchNorm1d(64)
        
        # é‡ã¿åˆæœŸåŒ–
        self._init_weights()
    
    def _init_weights(self):
        """é‡ã¿ã®åˆæœŸåŒ–"""
        for module in self.modules():
            if isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        torch.nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        torch.nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)
            elif isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x):
        # LSTMå±¤
        if self.model_type == "simple":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
            
        elif self.model_type == "bidirectional":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
            
        elif self.model_type == "stacked":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm3(lstm_out)
        
        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        
        # Denseå±¤
        out = F.relu(self.fc1(lstm_out))
        out = self.bn1(out)
        out = self.dropout(out)
        
        out = F.relu(self.fc2(out))
        out = self.bn2(out)
        out = self.dropout(out)
        
        out = self.fc_out(out)
        
        return out

class TennisLSTMTrainer:
    """
    PyTorch LSTMæ™‚ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ãƒ‹ã‚¹å±€é¢åˆ†é¡å­¦ç¿’ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, training_data_dir: str = "training_data"):
        self.training_data_dir = Path(training_data_dir)
        self.models_dir = self.training_data_dir / "lstm_models"
        self.models_dir.mkdir(exist_ok=True)
        
        self.phase_labels = [
            "point_interval",           # 0: ãƒã‚¤ãƒ³ãƒˆé–“
            "rally",                   # 1: ãƒ©ãƒªãƒ¼ä¸­
            "serve_preparation",       # 2: ã‚µãƒ¼ãƒ–æº–å‚™
            "serve_front_deuce",      # 3: æ‰‹å‰ãƒ‡ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒ‰ã‹ã‚‰ã®ã‚µãƒ¼ãƒ–
            "serve_front_ad",         # 4: æ‰‹å‰ã‚¢ãƒ‰ã‚µã‚¤ãƒ‰ã‹ã‚‰ã®ã‚µãƒ¼ãƒ–
            "serve_back_deuce",       # 5: å¥¥ãƒ‡ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒ‰ã‹ã‚‰ã®ã‚µãƒ¼ãƒ–
            "serve_back_ad",          # 6: å¥¥ã‚¢ãƒ‰ã‚µã‚¤ãƒ‰ã‹ã‚‰ã®ã‚µãƒ¼ãƒ–
            "changeover"              # 7: ãƒã‚§ãƒ³ã‚¸ã‚³ãƒ¼ãƒˆé–“
        ]
        
        # LSTMè¨­å®šï¼ˆGPUä½¿ç”¨æ™‚ã¯è¨­å®šã‚’èª¿æ•´ï¼‰
        if GPU_AVAILABLE:
            self.sequence_length = 30  # GPUä½¿ç”¨æ™‚ã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚‚å‡¦ç†å¯èƒ½
            self.overlap_ratio = 0.5
            self.lstm_units = [256, 128]  # GPUä½¿ç”¨æ™‚ã¯ã‚ˆã‚Šå¤§ããªãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            self.dense_units = [128, 64]
            self.dropout_rate = 0.3
            self.learning_rate = 0.001
            self.batch_size = 64  # GPUä½¿ç”¨æ™‚ã¯ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
        else:
            self.sequence_length = 20  # CPUä½¿ç”¨æ™‚ã¯è»½é‡åŒ–
            self.overlap_ratio = 0.5
            self.lstm_units = [128, 64]
            self.dense_units = [64, 32]
            self.dropout_rate = 0.3
            self.learning_rate = 0.001
            self.batch_size = 32
        
        self.scaler = None
        self.label_encoder = None
        self.model = None
        self.history = None
        
        print(f"PyTorch LSTMå±€é¢åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å™¨ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.training_data_dir}")
        print(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {self.models_dir}")
        print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {self.sequence_length}ãƒ•ãƒ¬ãƒ¼ãƒ ")
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
    
    def load_dataset(self, csv_path: str = None) -> Tuple[pd.DataFrame, bool]:
        """ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        if csv_path:
            dataset_path = Path(csv_path)
        else:
            # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œç´¢
            dataset_files = list(self.training_data_dir.glob("tennis_features_dataset_*.csv"))
            if not dataset_files:
                print("âŒ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print("feature_extractor.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„")
                return pd.DataFrame(), False
            
            dataset_path = max(dataset_files, key=lambda x: x.stat().st_mtime)
        
        try:
            df = pd.read_csv(dataset_path)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {dataset_path.name}")
            print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}")
            print(f"   ç‰¹å¾´é‡æ•°: {len(df.columns)}")
            
            # åŸºæœ¬æƒ…å ±è¡¨ç¤º
            self.analyze_dataset(df)
            
            return df, True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame(), False
    
    def analyze_dataset(self, df: pd.DataFrame):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŸºæœ¬åˆ†æ"""
        print(f"\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ ===")
        
        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
        print("ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        label_counts = df['label'].value_counts().sort_index()
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯
        unique_labels = sorted(df['label'].unique())
        max_label = max(unique_labels) if unique_labels else -1
        
        print(f"ãƒ©ãƒ™ãƒ«å€¤ã®ç¯„å›²: {min(unique_labels)} ï½ {max_label}")
        print(f"æœŸå¾…ã•ã‚Œã‚‹æœ€å¤§ãƒ©ãƒ™ãƒ«å€¤: {len(self.phase_labels) - 1}")
        
        # ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        invalid_labels = [label for label in unique_labels if label >= len(self.phase_labels) or label < 0]
        if invalid_labels:
            print(f"âš ï¸  ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {invalid_labels}")
            print("ã“ã‚Œã‚‰ã®ãƒ©ãƒ™ãƒ«ã¯å­¦ç¿’ã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™")
        
        for label_id, count in label_counts.items():
            if label_id < len(self.phase_labels) and label_id >= 0:
                phase_name = self.phase_labels[label_id]
                percentage = (count / len(df)) * 100
                print(f"  {label_id} ({phase_name}): {count} ({percentage:.1f}%)")
            else:
                print(f"  {label_id} (ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«): {count}")
        
        # å‹•ç”»åˆ¥åˆ†å¸ƒ
        print(f"\nå‹•ç”»æ•°: {df['video_name'].nunique()}")
        video_counts = df['video_name'].value_counts()
        print("å‹•ç”»åˆ¥ãƒ•ãƒ¬ãƒ¼ãƒ æ•°:")
        for video, count in video_counts.head(10).items():
            print(f"  {video}: {count}ãƒ•ãƒ¬ãƒ¼ãƒ ")
        
        # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—åˆ†æ
        feature_types = {
            'basic': [],
            'temporal': [],
            'court': [],
            'contextual': []
        }
        
        for col in df.columns:
            if col in ['label', 'video_name', 'frame_number']:
                continue
            elif any(suffix in col for suffix in ['_ma_', '_std_', '_diff', '_trend_', '_cv_']):
                feature_types['temporal'].append(col)
            elif 'court' in col or 'distance_to' in col:
                feature_types['court'].append(col)
            elif any(keyword in col for keyword in ['activity', 'interaction', 'stability', 'dynamics']):
                feature_types['contextual'].append(col)
            else:
                feature_types['basic'].append(col)
        
        print(f"\nç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—åˆ¥:")
        for ftype, features in feature_types.items():
            print(f"  {ftype}: {len(features)}ç‰¹å¾´é‡")
    
    def prepare_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        print(f"\n=== æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ ===")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£
        df_cleaned = self.validate_and_clean_labels(df)
        
        # ç‰¹å¾´é‡åˆ—ã‚’é¸æŠ
        exclude_columns = ['label', 'video_name', 'frame_number']
        feature_columns = [col for col in df_cleaned.columns if col not in exclude_columns]
        
        # æ•°å€¤ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨
        numeric_features = df_cleaned[feature_columns].select_dtypes(include=[np.number]).columns
        X_features = df_cleaned[numeric_features].values
        y_labels = df_cleaned['label'].values
        video_names = df_cleaned['video_name'].values
        
        print(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(numeric_features)}")
        print(f"ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(X_features)}")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        unique_labels = np.unique(y_labels)
        max_label = np.max(unique_labels)
        expected_num_classes = len(self.phase_labels)
        
        print(f"å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«å€¤ç¯„å›²: {np.min(unique_labels)} ï½ {max_label}")
        print(f"æœŸå¾…ã•ã‚Œã‚‹ã‚¯ãƒ©ã‚¹æ•°: {expected_num_classes}")
        
        if max_label >= expected_num_classes:
            print(f"âŒ ãƒ©ãƒ™ãƒ«å€¤ã‚¨ãƒ©ãƒ¼: æœ€å¤§ãƒ©ãƒ™ãƒ«å€¤ {max_label} ãŒã‚¯ãƒ©ã‚¹æ•° {expected_num_classes} ã‚’è¶…ãˆã¦ã„ã¾ã™")
            print("ãƒ‡ãƒ¼ã‚¿ã‚’å†ç¢ºèªã—ã¦ãã ã•ã„")
            return np.array([]), np.array([]), []
        
        # ç„¡é™å€¤ã¨NaNã‚’å‡¦ç†
        X_features = np.nan_to_num(X_features, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # æ¨™æº–åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X_features)
        
        # å‹•ç”»ã”ã¨ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
        sequences = []
        sequence_labels = []
        sequence_videos = []
        
        unique_videos = df_cleaned['video_name'].unique()
        
        for video in unique_videos:
            video_mask = video_names == video
            video_features = X_scaled[video_mask]
            video_labels = y_labels[video_mask]
            
            # å‹•ç”»å†…ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
            video_sequences, video_seq_labels = self.create_video_sequences(
                video_features, video_labels
            )
            
            sequences.extend(video_sequences)
            sequence_labels.extend(video_seq_labels)
            sequence_videos.extend([video] * len(video_sequences))
            
            print(f"  {video}: {len(video_sequences)}ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ")
        
        X_sequences = np.array(sequences)
        y_sequences = np.array(sequence_labels)
        
        # æœ€çµ‚çš„ãªãƒ©ãƒ™ãƒ«å€¤ãƒã‚§ãƒƒã‚¯
        final_unique_labels = np.unique(y_sequences)
        final_max_label = np.max(final_unique_labels) if len(final_unique_labels) > 0 else -1
        
        print(f"ä½œæˆã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(X_sequences)}")
        print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢çŠ¶: {X_sequences.shape}")
        print(f"æœ€çµ‚ãƒ©ãƒ™ãƒ«å€¤ç¯„å›²: {np.min(final_unique_labels)} ï½ {final_max_label}")
        
        return X_sequences, y_sequences, list(numeric_features)
    
    def validate_and_clean_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ©ãƒ™ãƒ«å€¤ã‚’æ¤œè¨¼ã—ã€ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«ã‚’é™¤å»"""
        print("ãƒ©ãƒ™ãƒ«å€¤ã®æ¤œè¨¼ã¨æ¸…æµ„åŒ–...")
        
        df_cleaned = df.copy()
        original_count = len(df_cleaned)
        
        # æœ‰åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ã®ç¯„å›²
        valid_label_range = range(len(self.phase_labels))
        
        # ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«ã‚’æŒã¤è¡Œã‚’ç‰¹å®š
        invalid_mask = ~df_cleaned['label'].isin(valid_label_range)
        invalid_count = invalid_mask.sum()
        
        if invalid_count > 0:
            print(f"âš ï¸  ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«ã‚’æŒã¤è¡Œæ•°: {invalid_count}/{original_count}")
            
            # ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ã®è©³ç´°
            invalid_labels = df_cleaned[invalid_mask]['label'].unique()
            print(f"ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤: {sorted(invalid_labels)}")
            
            # ç„¡åŠ¹ãªè¡Œã‚’é™¤å»
            df_cleaned = df_cleaned[~invalid_mask].copy()
            print(f"é™¤å»å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(df_cleaned)}/{original_count}")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã‚’é€£ç¶šã—ãŸæ•´æ•°ã«å†ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        unique_labels = sorted(df_cleaned['label'].unique())
        if unique_labels != list(range(len(unique_labels))):
            print("ãƒ©ãƒ™ãƒ«å€¤ã‚’é€£ç¶šã—ãŸæ•´æ•°ã«å†ãƒãƒƒãƒ”ãƒ³ã‚°...")
            
            # å…ƒã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰æ–°ã—ã„ãƒ©ãƒ™ãƒ«ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°
            label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}
            df_cleaned['label'] = df_cleaned['label'].map(label_mapping)
            
            print("ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°:")
            for old_label, new_label in label_mapping.items():
                if old_label < len(self.phase_labels):
                    phase_name = self.phase_labels[old_label]
                    print(f"  {old_label} ({phase_name}) -> {new_label}")
            
            # phase_labelsã‚’æ›´æ–°
            self.phase_labels = [self.phase_labels[old_label] for old_label in unique_labels 
                               if old_label < len(self.phase_labels)]
            print(f"ä½¿ç”¨ã•ã‚Œã‚‹å±€é¢ãƒ©ãƒ™ãƒ«: {self.phase_labels}")
        
        # æœ€çµ‚æ¤œè¨¼
        final_unique_labels = sorted(df_cleaned['label'].unique())
        expected_labels = list(range(len(self.phase_labels)))
        
        if final_unique_labels != expected_labels:
            print(f"âš ï¸  ãƒ©ãƒ™ãƒ«å€¤ãŒæœŸå¾…å€¤ã¨ä¸€è‡´ã—ã¾ã›ã‚“")
            print(f"å®Ÿéš›: {final_unique_labels}")
            print(f"æœŸå¾…: {expected_labels}")
        else:
            print(f"âœ… ãƒ©ãƒ™ãƒ«å€¤ã®æ¤œè¨¼å®Œäº†: {final_unique_labels}")
        
        return df_cleaned
    
    def create_video_sequences(self, features: np.ndarray, labels: np.ndarray) -> Tuple[List, List]:
        """å˜ä¸€å‹•ç”»ã‹ã‚‰ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ"""
        sequences = []
        sequence_labels = []
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
        step_size = max(1, int(self.sequence_length * (1 - self.overlap_ratio)))
        
        for i in range(0, len(features) - self.sequence_length + 1, step_size):
            seq_features = features[i:i + self.sequence_length]
            seq_labels = labels[i:i + self.sequence_length]
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¾Œã®ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ï¼ˆäºˆæ¸¬å¯¾è±¡ï¼‰
            target_label = seq_labels[-1]
            
            # æœ‰åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ã®ã¿ã‚’ä½¿ç”¨
            if 0 <= target_label < len(self.phase_labels):
                sequences.append(seq_features)
                sequence_labels.append(target_label)
        
        return sequences, sequence_labels
    
    def build_lstm_model(self, input_size: int, num_classes: int, model_type: str = "bidirectional") -> TennisLSTMModel:
        """PyTorch LSTMãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰"""
        print(f"\n=== {model_type.upper()} PyTorch LSTMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
        print(f"å…¥åŠ›ç‰¹å¾´é‡æ•°: {input_size}")
        print(f"å‡ºåŠ›ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")
        
        # ã‚¯ãƒ©ã‚¹æ•°ã®æ¤œè¨¼
        if num_classes <= 1:
            raise ValueError(f"ç„¡åŠ¹ãªã‚¯ãƒ©ã‚¹æ•°: {num_classes}. 2ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        model = TennisLSTMModel(
            input_size=input_size,
            hidden_sizes=self.lstm_units,
            num_classes=num_classes,
            dropout_rate=self.dropout_rate,
            model_type=model_type
        )
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•
        model = model.to(DEVICE)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†:")
        print(f"   ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
        print(f"   å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {trainable_params:,}")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        
        return model
    
    def train_model(self, X: np.ndarray, y: np.ndarray, model_type: str = "bidirectional") -> Dict:
        """PyTorchãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œ"""
        print(f"\n=== PyTorch LSTMå­¦ç¿’é–‹å§‹ ===")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®æœ€çµ‚æ¤œè¨¼
        unique_labels = np.unique(y)
        num_classes = len(unique_labels)
        max_label = np.max(unique_labels)
        
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«æƒ…å ±:")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {sorted(unique_labels)}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")
        print(f"  æœ€å¤§ãƒ©ãƒ™ãƒ«å€¤: {max_label}")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if max_label >= num_classes:
            raise ValueError(f"ãƒ©ãƒ™ãƒ«å€¤ã‚¨ãƒ©ãƒ¼: æœ€å¤§ãƒ©ãƒ™ãƒ«å€¤ {max_label} ãŒã‚¯ãƒ©ã‚¹æ•° {num_classes} ä»¥ä¸Šã§ã™")
        
        if min(unique_labels) < 0:
            raise ValueError(f"ãƒ©ãƒ™ãƒ«å€¤ã‚¨ãƒ©ãƒ¼: è² ã®ãƒ©ãƒ™ãƒ«å€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {min(unique_labels)}")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
        print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape}")
        print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
        
        # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)
        y_train_tensor = torch.LongTensor(y_train).to(DEVICE)
        X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)
        y_val_tensor = torch.LongTensor(y_val).to(DEVICE)
        X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)
        y_test_tensor = torch.LongTensor(y_test).to(DEVICE)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        input_size = X.shape[2]  # ç‰¹å¾´é‡æ•°
        self.model = self.build_lstm_model(input_size, num_classes, model_type)
        
        # æå¤±é–¢æ•°ã¨æœ€é©åŒ–é–¢æ•°
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-4)
        
        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆverboseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼‰
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
        
        # æ··åˆç²¾åº¦å­¦ç¿’ã®æº–å‚™
        scaler = GradScaler() if GPU_AVAILABLE else None
        
        # å­¦ç¿’å±¥æ­´
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'lr': []
        }
        
        # æ—©æœŸåœæ­¢ã®è¨­å®š
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 25 if GPU_AVAILABLE else 15
        
        # ã‚¨ãƒãƒƒã‚¯æ•°
        epochs = 150 if GPU_AVAILABLE else 100
        
        print("å­¦ç¿’é–‹å§‹...")
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        print(f"æ··åˆç²¾åº¦å­¦ç¿’: {'æœ‰åŠ¹' if GPU_AVAILABLE else 'ç„¡åŠ¹'}")
        
        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        for epoch in range(epochs):
            # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
            self.model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                
                if GPU_AVAILABLE and scaler:
                    # æ··åˆç²¾åº¦å­¦ç¿’
                    with autocast():
                        outputs = self.model(batch_X)
                        loss = criterion(outputs, batch_y)
                    
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    # é€šå¸¸ã®å­¦ç¿’
                    outputs = self.model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += batch_y.size(0)
                train_correct += (predicted == batch_y).sum().item()
            
            train_loss = train_loss / len(train_loader)
            train_acc = train_correct / train_total
            
            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
            self.model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    if GPU_AVAILABLE:
                        with autocast():
                            outputs = self.model(batch_X)
                            loss = criterion(outputs, batch_y)
                    else:
                        outputs = self.model(batch_X)
                        loss = criterion(outputs, batch_y)
                    
                    val_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    val_total += batch_y.size(0)
                    val_correct += (predicted == batch_y).sum().item()
            
            val_loss = val_loss / len(val_loader)
            val_acc = val_correct / val_total
            
            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆæ‰‹å‹•ã§verboseå‡ºåŠ›ï¼‰
            old_lr = optimizer.param_groups[0]['lr']
            scheduler.step(val_loss)
            new_lr = optimizer.param_groups[0]['lr']
            
            # å­¦ç¿’ç‡å¤‰æ›´æ™‚ã®å‡ºåŠ›
            if new_lr != old_lr:
                print(f"Epoch {epoch+1}: å­¦ç¿’ç‡ã‚’ {old_lr:.2e} ã‹ã‚‰ {new_lr:.2e} ã«å‰Šæ¸›")
            
            # å±¥æ­´ã‚’ä¿å­˜
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['lr'].append(optimizer.param_groups[0]['lr'])
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            if (epoch + 1) % 10 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], '
                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '
                      f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, '
                      f'LR: {optimizer.param_groups[0]["lr"]:.2e}')
            
            # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
                torch.save(self.model.state_dict(), self.models_dir / 'best_pytorch_model.pth')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"æ—©æœŸåœæ­¢: {patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                    break
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.model.load_state_dict(torch.load(self.models_dir / 'best_pytorch_model.pth'))
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        self.model.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0
        all_predictions = []
        all_probabilities = []
        all_true_labels = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                if GPU_AVAILABLE:
                    with autocast():
                        outputs = self.model(batch_X)
                        loss = criterion(outputs, batch_y)
                else:
                    outputs = self.model(batch_X)
                    loss = criterion(outputs, batch_y)
                
                test_loss += loss.item()
                probabilities = F.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                
                test_total += batch_y.size(0)
                test_correct += (predicted == batch_y).sum().item()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
                all_true_labels.extend(batch_y.cpu().numpy())
        
        test_loss = test_loss / len(test_loader)
        test_accuracy = test_correct / test_total
        
        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        y_test_np = np.array(all_true_labels)
        y_pred_np = np.array(all_predictions)
        y_pred_proba_np = np.array(all_probabilities)
        
        f1 = f1_score(y_test_np, y_pred_np, average='weighted')
        
        # å±¥æ­´ã‚’ä¿å­˜
        self.history = history
        
        results = {
            'test_accuracy': test_accuracy,
            'test_loss': test_loss,
            'f1_score': f1,
            'y_test': y_test_np,
            'y_pred': y_pred_np,
            'y_pred_proba': y_pred_proba_np,
            'model_type': model_type,
            'gpu_used': GPU_AVAILABLE,
            'history': history
        }
        
        print(f"\n=== å­¦ç¿’çµæœ ===")
        print(f"ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f}")
        print(f"ãƒ†ã‚¹ãƒˆæå¤±: {test_loss:.4f}")
        print(f"F1ã‚¹ã‚³ã‚¢: {f1:.4f}")
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        
        if GPU_AVAILABLE:
            print("âœ… GPUåŠ é€Ÿã«ã‚ˆã‚‹é«˜é€Ÿå­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
        return results
    
    def compare_models(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """è¤‡æ•°ã®LSTMãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ"""
        print(f"\n=== LSTM ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ===")
        
        model_types = ["simple", "bidirectional", "stacked"]
        results = {}
        
        for model_type in model_types:
            print(f"\n--- {model_type.upper()} LSTM ---")
            try:
                result = self.train_model(X, y, model_type)
                results[model_type] = result
                
                print(f"{model_type} å®Œäº†:")
                print(f"  ç²¾åº¦: {result['test_accuracy']:.4f}")
                print(f"  F1: {result['f1_score']:.4f}")
                
            except Exception as e:
                print(f"âŒ {model_type} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_type] = {'error': str(e)}
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        valid_results = {name: result for name, result in results.items() 
                        if 'error' not in result}
        
        if valid_results:
            best_model = max(valid_results.keys(), 
                           key=lambda x: valid_results[x]['test_accuracy'])
            
            print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model.upper()}")
            print(f"   ç²¾åº¦: {valid_results[best_model]['test_accuracy']:.4f}")
            
            results['best_model'] = best_model
        
        return results
    
    def plot_training_history(self):
        """å­¦ç¿’å±¥æ­´ã‚’å¯è¦–åŒ–"""
        if self.history is None:
            print("å­¦ç¿’å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æå¤±
        axes[0, 0].plot(self.history['train_loss'], label='Training Loss')
        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # ç²¾åº¦
        axes[0, 1].plot(self.history['train_acc'], label='Training Accuracy')
        axes[0, 1].plot(self.history['val_acc'], label='Validation Accuracy')
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å­¦ç¿’ç‡ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
        if 'lr' in self.history:
            axes[1, 0].plot(self.history['lr'])
            axes[1, 0].set_title('Learning Rate')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Learning Rate')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True)
        
        # æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®æƒ…å ±
        final_epoch = len(self.history['train_loss'])
        final_train_acc = self.history['train_acc'][-1]
        final_val_acc = self.history['val_acc'][-1]
        
        axes[1, 1].text(0.1, 0.7, f'Final Epoch: {final_epoch}', fontsize=12)
        axes[1, 1].text(0.1, 0.5, f'Final Train Acc: {final_train_acc:.4f}', fontsize=12)
        axes[1, 1].text(0.1, 0.3, f'Final Val Acc: {final_val_acc:.4f}', fontsize=12)
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(self.models_dir / f'lstm_training_history_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray):
        """æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.phase_labels, yticklabels=self.phase_labels)
        plt.title('LSTM Model - Confusion Matrix')
        plt.xlabel('Predicted Phase')
        plt.ylabel('True Phase')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(self.models_dir / f'lstm_confusion_matrix_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def evaluate_model(self, results: Dict):
        """è©³ç´°ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        print(f"\n=== è©³ç´°è©•ä¾¡ ===")
        
        y_test = results['y_test']
        y_pred = results['y_pred']
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        target_names = [self.phase_labels[i] for i in sorted(np.unique(y_test))]
        print("åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(classification_report(y_test, y_pred, target_names=target_names))
        
        # æ··åŒè¡Œåˆ—ã‚’å¯è¦–åŒ–
        self.plot_confusion_matrix(y_test, y_pred)
        
        # å­¦ç¿’å±¥æ­´ã‚’å¯è¦–åŒ–
        self.plot_training_history()
    
    def save_model(self, results: Dict, feature_names: List[str]) -> Dict[str, str]:
        """å­¦ç¿’æ¸ˆã¿PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model_path = self.models_dir / f"tennis_pytorch_model_{timestamp}.pth"
        scaler_path = self.models_dir / f"tennis_pytorch_scaler_{timestamp}.pkl"
        metadata_path = self.models_dir / f"tennis_pytorch_metadata_{timestamp}.json"
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆçŠ¶æ…‹è¾æ›¸ã¨ãƒ¢ãƒ‡ãƒ«æ§‹é€ ï¼‰
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.input_size,
                'hidden_sizes': self.model.hidden_sizes,
                'num_classes': self.model.num_classes,
                'dropout_rate': self.model.dropout_rate,
                'model_type': self.model.model_type
            }
        }, model_path)
        
        joblib.dump(self.scaler, scaler_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        metadata = {
            'creation_time': timestamp,
            'framework': 'pytorch',
            'model_type': results.get('model_type', 'bidirectional'),
            'test_accuracy': float(results['test_accuracy']),
            'test_loss': float(results['test_loss']),
            'f1_score': float(results['f1_score']),
            'phase_labels': self.phase_labels,
            'feature_names': feature_names,
            'sequence_length': self.sequence_length,
            'overlap_ratio': self.overlap_ratio,
            'lstm_units': self.lstm_units,
            'dense_units': self.dense_units,
            'dropout_rate': self.dropout_rate,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'feature_count': len(feature_names),
            'device': str(DEVICE),
            'gpu_used': results.get('gpu_used', False),
            'cuda_version': torch.version.cuda if GPU_AVAILABLE else None,
            'pytorch_version': torch.__version__
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\n=== PyTorch ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº† ===")
        print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_path}")
        print(f"ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {scaler_path}")
        print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path}")
        print(f"âš¡ GPUä½¿ç”¨: {'ã¯ã„' if results.get('gpu_used', False) else 'ã„ã„ãˆ'}")
        
        return {
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'metadata_path': str(metadata_path)
        }
    
    def train_complete_pipeline(self, csv_path: str = None, model_comparison: bool = True) -> bool:
        """å®Œå…¨ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print("=== ãƒ†ãƒ‹ã‚¹å±€é¢åˆ†é¡LSTMå­¦ç¿’é–‹å§‹ ===")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df, success = self.load_dataset(csv_path)
        if not success:
            return False
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        try:
            X, y, feature_names = self.prepare_sequences(df)
            print(f"âœ… ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆå®Œäº†: {X.shape}")
        except Exception as e:
            print(f"âŒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        try:
            if model_comparison:
                # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
                all_results = self.compare_models(X, y)
                
                if 'best_model' in all_results:
                    best_model_type = all_results['best_model']
                    results = all_results[best_model_type]
                    
                    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§å†å­¦ç¿’
                    print(f"\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«({best_model_type})ã§æœ€çµ‚å­¦ç¿’...")
                    results = self.train_model(X, y, best_model_type)
                else:
                    print("âŒ æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                    return False
            else:
                # å˜ä¸€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                results = self.train_model(X, y, "bidirectional")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # è©³ç´°è©•ä¾¡
        try:
            self.evaluate_model(results)
        except Exception as e:
            print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        try:
            save_info = self.save_model(results, feature_names)
            print(f"\nğŸ‰ LSTMå­¦ç¿’å®Œäº†ï¼")
            print(f"æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã§ãã¾ã™")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def check_cuda_installation():
    """CUDAç’°å¢ƒã‚’ãƒã‚§ãƒƒã‚¯"""
    print("=== CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼ˆPyTorchï¼‰ ===")
    
    # PyTorchã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    print(f"PyTorchç‰ˆ: {torch.__version__}")
    
    # CUDAåˆ©ç”¨å¯èƒ½æ€§
    cuda_available = torch.cuda.is_available()
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {'ã¯ã„' if cuda_available else 'ã„ã„ãˆ'}")
    
    if cuda_available:
        # GPUæƒ…å ±
        device_count = torch.cuda.device_count()
        print(f"æ¤œå‡ºGPUæ•°: {device_count}")
        
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        
        # CUDAè©³ç´°æƒ…å ±
        cuda_version = torch.version.cuda
        print(f"CUDAç‰ˆ: {cuda_version}")
        
        # cuDNNæƒ…å ±
        cudnn_available = torch.backends.cudnn.is_available()
        if cudnn_available:
            cudnn_version = torch.backends.cudnn.version()
            print(f"cuDNNç‰ˆ: {cudnn_version}")
        
        # æ··åˆç²¾åº¦ã‚µãƒãƒ¼ãƒˆ
        if torch.cuda.is_bf16_supported():
            print("æ··åˆç²¾åº¦: BF16å¯¾å¿œ")
        else:
            print("æ··åˆç²¾åº¦: FP16å¯¾å¿œ")
    
    return cuda_available

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=== ãƒ†ãƒ‹ã‚¹å‹•ç”»å±€é¢åˆ†é¡PyTorch LSTMå­¦ç¿’ãƒ„ãƒ¼ãƒ« ===")
    
    # CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯
    cuda_ok = check_cuda_installation()
    
    # å­¦ç¿’å™¨ã‚’åˆæœŸåŒ–
    trainer = TennisLSTMTrainer()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
    dataset_files = list(trainer.training_data_dir.glob("tennis_features_dataset_*.csv"))
    print(f"\n=== ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª ===")
    print(f"ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(dataset_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    if not dataset_files:
        print("\nâŒ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("feature_extractor.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„")
        return
    
    for file in dataset_files:
        print(f"  - {file.name}")
    
    # GPUä½¿ç”¨çŠ¶æ³ã®æœ€çµ‚ç¢ºèª
    if GPU_AVAILABLE:
        print(f"\nğŸš€ PyTorch GPUåŠ é€Ÿå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
        print(f"   æ··åˆç²¾åº¦å­¦ç¿’: æœ‰åŠ¹")
        print(f"   CuDNNæœ€é©åŒ–: æœ‰åŠ¹")
    else:
        print(f"\nğŸ’» PyTorch CPUå­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™")
        if not cuda_ok:
            print("   GPU/CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            print("   1. NVIDIA GPUï¼ˆCUDAå¯¾å¿œï¼‰ãŒæ­è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹")
            print("   2. CUDAãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹")
            print("   3. PyTorch CUDAç‰ˆãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹")
    
    try:
        # å®Œå…¨ãªå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        success = trainer.train_complete_pipeline(model_comparison=True)
        
        if success:
            print(f"\nâœ… å…¨ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            if GPU_AVAILABLE:
                print("ğŸš€ PyTorch GPUåŠ é€Ÿã«ã‚ˆã‚Šé«˜é€Ÿå­¦ç¿’ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸ")
        else:
            print(f"\nâŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
