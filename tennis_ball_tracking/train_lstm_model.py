import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import joblib

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import GradScaler, autocast

# GPUè¨­å®š
def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"ğŸš€ GPUæ¤œå‡º: {device_count}å°")
        
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        
        print(f"âœ… CUDA version: {torch.version.cuda}")
        print(f"âœ… cuDNN version: {torch.backends.cudnn.version()}")
        
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        return torch.device('cuda')
    else:
        print("âš ï¸  GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
        return torch.device('cpu')

DEVICE = setup_gpu_config()
GPU_AVAILABLE = DEVICE.type == 'cuda'

plt.style.use('default')
sns.set_palette("husl")

class TennisLSTMModel(nn.Module):
    """PyTorch LSTM ãƒ¢ãƒ‡ãƒ«ï¼ˆä¿¡é ¼åº¦é‡ã¿ä»˜ã‘å¯¾å¿œï¼‰"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, 
                 dropout_rate: float = 0.3, model_type: str = "bidirectional", 
                 use_batch_norm: bool = True, enable_confidence_weighting: bool = True):
        super(TennisLSTMModel, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.model_type = model_type
        self.use_batch_norm = use_batch_norm
        self.enable_confidence_weighting = enable_confidence_weighting
        
        # ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘ç”¨ã®æ³¨æ„æ©Ÿæ§‹
        if self.enable_confidence_weighting:
            self.confidence_attention = nn.Sequential(
                nn.Linear(input_size, hidden_sizes[0] // 4),
                nn.ReLU(),
                nn.Linear(hidden_sizes[0] // 4, 1),
                nn.Sigmoid()
            )
        
        # LSTMå±¤ã‚’æ§‹ç¯‰
        self._build_lstm_layers()
        
        # å‡ºåŠ›å±¤
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(self.lstm_output_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc_out = nn.Linear(64, num_classes)
        
        # æ­£è¦åŒ–å±¤
        if self.use_batch_norm:
            self.bn1 = nn.BatchNorm1d(128)
            self.bn2 = nn.BatchNorm1d(64)
        else:
            self.ln1 = nn.LayerNorm(128)
            self.ln2 = nn.LayerNorm(64)
        
        self._init_weights()
    
    def _build_lstm_layers(self):
        """LSTMå±¤ã®æ§‹ç¯‰"""
        if self.model_type == "simple":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[1], batch_first=True, dropout=self.dropout_rate)
            self.lstm_output_size = self.hidden_sizes[1]
            
        elif self.model_type == "bidirectional":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, 
                                bidirectional=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0] * 2, self.hidden_sizes[1], batch_first=True,
                                bidirectional=True, dropout=self.dropout_rate)
            self.lstm_output_size = self.hidden_sizes[1] * 2
            
        elif self.model_type == "stacked":
            self.lstm1 = nn.LSTM(self.input_size, self.hidden_sizes[0], batch_first=True, dropout=self.dropout_rate)
            self.lstm2 = nn.LSTM(self.hidden_sizes[0], self.hidden_sizes[1], batch_first=True, dropout=self.dropout_rate)
            self.lstm3 = nn.LSTM(self.hidden_sizes[1], 64, batch_first=True, dropout=self.dropout_rate)
            self.lstm_output_size = 64
    
    def _init_weights(self):
        """é‡ã¿ã®åˆæœŸåŒ–"""
        for module in self.modules():
            if isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight_ih' in name:
                        torch.nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        torch.nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)
            elif isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x, confidence_scores=None):
        """ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹"""
        # ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘
        if self.enable_confidence_weighting and confidence_scores is not None:
            attention_weights = self.confidence_attention(x).squeeze(-1)
            combined_weights = attention_weights * confidence_scores
            x = x * combined_weights.unsqueeze(-1)
        
        # LSTMå±¤
        lstm_out = self._forward_lstm(x)
        
        # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        
        # Denseå±¤
        out = F.relu(self.fc1(lstm_out))
        out = self._apply_normalization(out, self.bn1 if self.use_batch_norm else self.ln1)
        out = self.dropout(out)
        
        out = F.relu(self.fc2(out))
        out = self._apply_normalization(out, self.bn2 if self.use_batch_norm else self.ln2)
        out = self.dropout(out)
        
        return self.fc_out(out)
    
    def _forward_lstm(self, x):
        """LSTMå±¤ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹"""
        if self.model_type == "simple":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
        elif self.model_type == "bidirectional":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
        elif self.model_type == "stacked":
            lstm_out, _ = self.lstm1(x)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm2(lstm_out)
            lstm_out = self.dropout(lstm_out)
            lstm_out, _ = self.lstm3(lstm_out)
        return lstm_out
    
    def _apply_normalization(self, x, norm_layer):
        """æ­£è¦åŒ–å±¤ã®é©ç”¨"""
        if self.use_batch_norm and x.size(0) > 1:
            return norm_layer(x)
        elif not self.use_batch_norm:
            return norm_layer(x)
        return x

class TennisLSTMTrainer:
    """ãƒ†ãƒ‹ã‚¹å±€é¢åˆ†é¡LSTMå­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, training_data_dir: str = "training_data"):
        self.training_data_dir = Path(training_data_dir)
        self.features_dir = self.training_data_dir / "features"
        self.models_dir = self.training_data_dir / "lstm_models"
        
        self._setup_directories()
        self._setup_parameters()
        
        self.phase_labels = [
            "point_interval", "rally", "serve_preparation",
            "serve_front_deuce", "serve_front_ad", "serve_back_deuce",
            "serve_back_ad", "changeover"
        ]
        
        self.scaler = None
        self.model = None
        self.history = None
        self.label_map: Optional[Dict[int, int]] = None
        self.active_phase_labels: Optional[List[str]] = None
        
        print(f"PyTorch LSTMå­¦ç¿’å™¨åˆæœŸåŒ–å®Œäº†")
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.training_data_dir}")
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
    
    def _setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š"""
        try:
            self.training_data_dir.mkdir(exist_ok=True)
            self.features_dir.mkdir(exist_ok=True)
            self.models_dir.mkdir(exist_ok=True)
            
            # æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ
            test_file = self.models_dir / "test_write.tmp"
            test_file.write_text("test")
            test_file.unlink()
            
        except Exception as e:
            print(f"âš ï¸  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            import tempfile
            self.models_dir = Path(tempfile.gettempdir()) / "tennis_lstm_models"
            self.models_dir.mkdir(exist_ok=True)
    
    def _setup_parameters(self):
        """å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š"""
        if GPU_AVAILABLE:
            self.sequence_length = 30
            self.overlap_ratio = 0.5
            self.lstm_units = [256, 128]
            self.dropout_rate = 0.3
            self.learning_rate = 0.001
            self.batch_size = 64
        else:
            self.sequence_length = 20
            self.overlap_ratio = 0.5
            self.lstm_units = [128, 64]
            self.dropout_rate = 0.3
            self.learning_rate = 0.001
            self.batch_size = 32
    
    def select_dataset_file(self) -> Optional[str]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®é¸æŠ"""
        # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        dataset_files = []
        
        # features ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æ¤œç´¢
        patterns = ["tennis_features_dataset_*.csv", "tennis_features_*.csv"]
        for pattern in patterns:
            dataset_files.extend(list(self.features_dir.glob(pattern)))
        
        # features ãƒ•ã‚©ãƒ«ãƒ€ã«ãªã„å ´åˆã¯ training_data ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰æ¤œç´¢
        if not dataset_files:
            for pattern in patterns:
                dataset_files.extend(list(self.training_data_dir.glob(pattern)))
        
        # é‡è¤‡ã‚’é™¤å»ï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œå‡ºã•ã‚Œã‚‹å ´åˆï¼‰
        dataset_files = list(set(dataset_files))
        
        if not dataset_files:
            print("âŒ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: tennis_features_dataset_*.csv, tennis_features_*.csv")
            return None
        
        dataset_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        print(f"\n=== ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        for i, file_path in enumerate(dataset_files, 1):
            stat = file_path.stat()
            size_mb = stat.st_size / (1024 * 1024)
            mtime = datetime.fromtimestamp(stat.st_mtime)
            
            print(f"{i}: {file_path.name}")
            print(f"   æ›´æ–°æ—¥æ™‚: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   ã‚µã‚¤ã‚º: {size_mb:.2f} MB")
        
        print(f"{len(dataset_files) + 1}: è‡ªå‹•é¸æŠï¼ˆæœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
        
        try:
            choice = input(f"\né¸æŠã—ã¦ãã ã•ã„ (1-{len(dataset_files) + 1}): ").strip()
            choice_num = int(choice)
            
            if 1 <= choice_num <= len(dataset_files):
                return str(dataset_files[choice_num - 1])
            elif choice_num == len(dataset_files) + 1:
                return str(dataset_files[0])
            else:
                return None
        except ValueError:
            return None
    
    def load_dataset(self, csv_path: str = None, sample_ratio: float = 1.0) -> Tuple[pd.DataFrame, bool]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿"""
        if csv_path:
            dataset_path = Path(csv_path)
        else:
            selected_path = self.select_dataset_file()
            if not selected_path:
                return pd.DataFrame(), False
            dataset_path = Path(selected_path)
        
        try:
            file_size_mb = dataset_path.stat().st_size / (1024 * 1024)
            print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: {dataset_path.name} ({file_size_mb:.1f}MB)")
            
            if sample_ratio < 1.0:
                df = self._load_sampled_data(dataset_path, sample_ratio)
            else:
                df = pd.read_csv(dataset_path)
            
            print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ, {len(df.columns)} ç‰¹å¾´é‡")
            self.analyze_dataset(df)
            return df, True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame(), False
    
    def _load_sampled_data(self, dataset_path: Path, sample_ratio: float) -> pd.DataFrame:
        """ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°èª­ã¿è¾¼ã¿"""
        with open(dataset_path, 'r') as f:
            total_lines = sum(1 for line in f) - 1
        
        sample_size = int(total_lines * sample_ratio)
        skip_rows = sorted(np.random.choice(
            range(1, total_lines + 1), 
            size=total_lines - sample_size, 
            replace=False
        ))
        
        return pd.read_csv(dataset_path, skiprows=skip_rows)
    
    def analyze_dataset(self, df: pd.DataFrame):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŸºæœ¬åˆ†æ"""
        print(f"\n=== ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ ===")
        print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df):,}")
        
        # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
        label_counts = df['label'].value_counts().sort_index()
        print("\nãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
        for label_id, count in label_counts.items():
            if 0 <= label_id < len(self.phase_labels):
                phase_name = self.phase_labels[label_id]
                percentage = (count / len(df)) * 100
                print(f"  {label_id} ({phase_name}): {count:,} ({percentage:.1f}%)")
        
        # å‹•ç”»æƒ…å ±
        if 'video_name' in df.columns:
            video_count = df['video_name'].nunique()
            print(f"\nå‹•ç”»æ•°: {video_count}")
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹äºˆæ¸¬
        if 'video_name' in df.columns:
            total_sequences = self._estimate_sequences(df)
            print(f"äºˆæƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {total_sequences:,}")
    
    def _estimate_sequences(self, df: pd.DataFrame) -> int:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°ã®äºˆæ¸¬"""
        total_sequences = 0
        step_size = max(1, int(self.sequence_length * (1 - self.overlap_ratio)))
        
        for video in df['video_name'].unique():
            video_data = df[df['video_name'] == video]
            video_frames = len(video_data)
            
            if video_frames >= self.sequence_length:
                sequences = max(0, (video_frames - self.sequence_length) // step_size + 1)
                total_sequences += sequences
        
        return total_sequences
    
    def _extract_confidence_scores(self, df: pd.DataFrame) -> np.ndarray:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æŠ½å‡º"""
        if 'interpolation_confidence' in df.columns:
            return df['interpolation_confidence'].fillna(1.0).values
        elif 'interpolated' in df.columns:
            interpolated = df['interpolated'].fillna(False).astype(bool)
            return np.where(interpolated, 0.7, 1.0)
        else:
            return np.ones(len(df))

    def _create_sequences(self, X_scaled: np.ndarray, y_labels: np.ndarray, 
                         video_names: np.ndarray, confidence_scores: np.ndarray) -> Tuple[List, List, List]:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä½œæˆ"""
        sequences = []
        sequence_labels = []
        confidence_sequences = []
        
        step_size = max(1, int(self.sequence_length * (1 - self.overlap_ratio)))
        
        for video in np.unique(video_names):
            video_mask = video_names == video
            video_X = X_scaled[video_mask]
            video_y = y_labels[video_mask]
            video_conf = confidence_scores[video_mask]
            
            # å‹•ç”»å†…ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
            for i in range(0, len(video_X) - self.sequence_length + 1, step_size):
                seq_X = video_X[i:i + self.sequence_length]
                seq_y = video_y[i:i + self.sequence_length]
                seq_conf = video_conf[i:i + self.sequence_length]
                
                target_label = seq_y[-1]
                if 0 <= target_label < len(self.phase_labels):
                    sequences.append(seq_X)
                    sequence_labels.append(target_label)
                    confidence_sequences.append(seq_conf)
        
        return sequences, sequence_labels, confidence_sequences

    def prepare_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[str]]:
        """æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆCUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰"""
        print(f"\n=== æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆï¼ˆCUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰ ===")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®æ¤œè¨¼ã¨ä¿®æ­£ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        df_cleaned = self._validate_and_clean_labels(df)
        
        if len(df_cleaned) == 0:
            print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return np.array([]), np.array([]), np.array([]), []
        
        # ç‰¹å¾´é‡æŠ½å‡º
        exclude_columns = ['label', 'video_name', 'frame_number', 'interpolated', 'interpolation_confidence']
        feature_columns = [col for col in df_cleaned.columns if col not in exclude_columns]
        numeric_features = df_cleaned[feature_columns].select_dtypes(include=[np.number]).columns
        
        print(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(numeric_features)}")
        
        X_features = df_cleaned[numeric_features].values
        y_labels = df_cleaned['label'].values
        video_names = df_cleaned['video_name'].values
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®æœ€çµ‚æ¤œè¨¼
        unique_labels = np.unique(y_labels)
        print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆå‰ã®ãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼:")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {sorted(unique_labels)}")
        print(f"  æœ€å°å€¤: {np.min(unique_labels)}, æœ€å¤§å€¤: {np.max(unique_labels)}")
        print(f"  æœŸå¾…ã•ã‚Œã‚‹æœ€å¤§å€¤: {len(self.phase_labels) - 1}")
        
        # CUDAç”¨ã®ãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼
        if np.min(unique_labels) < 0:
            print(f"âŒ è² ã®ãƒ©ãƒ™ãƒ«å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {np.min(unique_labels)}")
            return np.array([]), np.array([]), np.array([]), []
        
        if np.max(unique_labels) >= len(self.phase_labels):
            print(f"âŒ ç¯„å›²å¤–ã®ãƒ©ãƒ™ãƒ«å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {np.max(unique_labels)} >= {len(self.phase_labels)}")
            return np.array([]), np.array([]), np.array([]), []
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢æŠ½å‡º
        confidence_scores = self._extract_confidence_scores(df_cleaned)
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X_features = np.nan_to_num(X_features, nan=0.0, posinf=1e6, neginf=-1e6)
        # self.scaler = StandardScaler() # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–ã‚’å‰Šé™¤
        # X_scaled = self.scaler.fit_transform(X_features) # ã“ã“ã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å‰Šé™¤
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        sequences, sequence_labels, confidence_sequences = self._create_sequences(
            X_features, y_labels, video_names, confidence_scores # X_scaled ã®ä»£ã‚ã‚Šã« X_features ã‚’ä½¿ç”¨
        )
        
        if len(sequences) == 0:
            print("âŒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return np.array([]), np.array([]), np.array([]), []
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¾Œã®ãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼
        final_unique_labels = np.unique(sequence_labels)
        print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆå¾Œã®ãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼:")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {sorted(final_unique_labels)}")
        print(f"  æœ€å°å€¤: {np.min(final_unique_labels)}, æœ€å¤§å€¤: {np.max(final_unique_labels)}")
        
        print(f"ä½œæˆã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {len(sequences)}")
        print(f"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å½¢çŠ¶: {np.array(sequences).shape}")
        print(f"âœ… CUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼šãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼å®Œäº†")
        
        return np.array(sequences), np.array(sequence_labels), np.array(confidence_sequences), list(numeric_features)
    
    def _validate_and_clean_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ©ãƒ™ãƒ«å€¤ã®æ¤œè¨¼ã¨ä¿®æ­£ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        print(f"\n=== ãƒ©ãƒ™ãƒ«å€¤æ¤œè¨¼ãƒ»ä¿®æ­£ï¼ˆå¼·åŒ–ç‰ˆï¼‰ ===")
        
        # å…ƒã®ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        original_count = len(df)
        unique_labels_before = sorted(df['label'].unique())
        print(f"ä¿®æ­£å‰ãƒ‡ãƒ¼ã‚¿æ•°: {original_count:,}")
        print(f"ä¿®æ­£å‰ãƒ©ãƒ™ãƒ«å€¤: {unique_labels_before}")
        print(f"ä¿®æ­£å‰ãƒ©ãƒ™ãƒ«å€¤ç¯„å›²: {min(unique_labels_before)} ï½ {max(unique_labels_before)}")
        
        # NaNå€¤ã®å‡¦ç†
        nan_mask = df['label'].isna()
        if nan_mask.sum() > 0:
            print(f"âš ï¸  NaNãƒ©ãƒ™ãƒ«æ¤œå‡º: {nan_mask.sum():,}è¡Œ - é™¤å»")
            df = df[~nan_mask].copy()
        
        # ç„¡é™å€¤ã®å‡¦ç†
        inf_mask = np.isinf(df['label'])
        if inf_mask.sum() > 0:
            print(f"âš ï¸  ç„¡é™å€¤ãƒ©ãƒ™ãƒ«æ¤œå‡º: {inf_mask.sum():,}è¡Œ - é™¤å»")
            df = df[~inf_mask].copy()
        
        # æœ‰åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ã®ç¯„å›²ï¼ˆ0 ï½ len(phase_labels)-1ï¼‰
        valid_range = list(range(len(self.phase_labels)))
        print(f"æœ‰åŠ¹ãƒ©ãƒ™ãƒ«ç¯„å›²: {valid_range}")
        
        # ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«ã‚’ç‰¹å®š
        invalid_mask = ~df['label'].isin(valid_range)
        invalid_count = invalid_mask.sum()
        
        if invalid_count > 0:
            invalid_labels = sorted(df[invalid_mask]['label'].unique())
            print(f"âš ï¸  ç„¡åŠ¹ãƒ©ãƒ™ãƒ«æ¤œå‡º: {invalid_count:,}è¡Œ")
            print(f"ç„¡åŠ¹ãƒ©ãƒ™ãƒ«å€¤: {invalid_labels}")
            
            # ç„¡åŠ¹ãƒ©ãƒ™ãƒ«ã®è©³ç´°åˆ†æ
            for invalid_label in invalid_labels:
                count = (df['label'] == invalid_label).sum()
                print(f"  ãƒ©ãƒ™ãƒ« {invalid_label}: {count:,}è¡Œ")
            
            # ç„¡åŠ¹ãƒ©ãƒ™ãƒ«ã‚’é™¤å»
            df = df[~invalid_mask].copy()
            print(f"âœ… ç„¡åŠ¹ãƒ©ãƒ™ãƒ«é™¤å»å®Œäº†")
        
        # æœ€çµ‚çµ±è¨ˆ
        final_count = len(df)
        if final_count > 0:
            unique_labels_after = sorted(df['label'].unique())
            print(f"ä¿®æ­£å¾Œãƒ‡ãƒ¼ã‚¿æ•°: {final_count:,}")
            print(f"ä¿®æ­£å¾Œãƒ©ãƒ™ãƒ«å€¤: {unique_labels_after}")
            print(f"ä¿®æ­£å¾Œãƒ©ãƒ™ãƒ«å€¤ç¯„å›²: {min(unique_labels_after)} ï½ {max(unique_labels_after)}")
            
            # ãƒ‡ãƒ¼ã‚¿ä¿æŒç‡
            retention_rate = (final_count / original_count) * 100
            print(f"ãƒ‡ãƒ¼ã‚¿ä¿æŒç‡: {retention_rate:.1f}%")
            
            # ãƒ©ãƒ™ãƒ«å€¤ã®å‹ã¨ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèª
            print(f"ãƒ©ãƒ™ãƒ«åˆ—ãƒ‡ãƒ¼ã‚¿å‹: {df['label'].dtype}")
            
            # æ•´æ•°å‹ã«å¤‰æ›ï¼ˆCUDAç”¨ï¼‰
            df['label'] = df['label'].astype(int)
            print(f"ãƒ©ãƒ™ãƒ«åˆ—ã‚’æ•´æ•°å‹ã«å¤‰æ›: {df['label'].dtype}")
            
        else:
            print("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã£ã¦ã„ã¾ã›ã‚“")
        
        return df

    def build_model(self, input_size: int, num_classes: int, model_type: str = "bidirectional", 
                   enable_confidence_weighting: bool = False) -> TennisLSTMModel:
        """PyTorch LSTMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰"""
        print(f"\n=== {model_type.upper()} PyTorch LSTMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ===")
        
        use_batch_norm = self.batch_size > 4
        
        model = TennisLSTMModel(
            input_size=input_size,
            hidden_sizes=self.lstm_units,
            num_classes=num_classes,
            dropout_rate=self.dropout_rate,
            model_type=model_type,
            use_batch_norm=use_batch_norm,
            enable_confidence_weighting=enable_confidence_weighting
        )
        
        model = model.to(DEVICE)
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†: {total_params:,}ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        
        return model
    
    def train_model(self, X: np.ndarray, y: np.ndarray, confidence_scores: np.ndarray = None, 
                   model_type: str = "bidirectional") -> Dict:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œï¼ˆCUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰"""
        print(f"\n=== PyTorch LSTMå­¦ç¿’é–‹å§‹ï¼ˆCUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰ ===")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚æ¤œè¨¼
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼:")
        print(f"  X shape: {X.shape}")
        print(f"  y shape: {y.shape}")
        
        # ãƒ©ãƒ™ãƒ«å€¤ã®è©³ç´°æ¤œè¨¼ã¨å†ãƒãƒƒãƒ”ãƒ³ã‚°
        original_unique_labels = sorted(np.unique(y))
        # ã‚­ãƒ¼ã‚’Pythonã®intå‹ã«å¤‰æ›
        self.label_map = {int(orig_label): i for i, orig_label in enumerate(original_unique_labels)}
        self.active_phase_labels = [self.phase_labels[i] for i in original_unique_labels if i < len(self.phase_labels)]
        
        y_remapped = np.array([self.label_map[label] for label in y])
        
        num_classes = len(original_unique_labels) # å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ã‚¯ãƒ©ã‚¹æ•°
        min_label_remapped = np.min(y_remapped)
        max_label_remapped = np.max(y_remapped)
        
        print(f"  ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {original_unique_labels}")
        print(f"  å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«: {sorted(np.unique(y_remapped))}")
        print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå±€é¢ãƒ©ãƒ™ãƒ«: {self.active_phase_labels}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•° (ãƒ¢ãƒ‡ãƒ«ç”¨): {num_classes}")
        print(f"  å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œãƒ©ãƒ™ãƒ«ç¯„å›²: {min_label_remapped} ï½ {max_label_remapped}")
        
        # CUDAã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼šãƒ©ãƒ™ãƒ«å€¤ã®æœ€çµ‚æ¤œè¨¼ (å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œ)
        if min_label_remapped < 0:
            raise ValueError(f"å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã€è² ã®ãƒ©ãƒ™ãƒ«å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {min_label_remapped}")
        
        # num_classes ã¯ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ãªã®ã§ã€ãƒ©ãƒ™ãƒ«ã¯ 0 ã‹ã‚‰ num_classes-1 ã®ç¯„å›²ã«ã‚ã‚‹ã¹ã
        if max_label_remapped >= num_classes:
            raise ValueError(
                f"å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ãƒ©ãƒ™ãƒ«å€¤ãŒç¯„å›²å¤–ã§ã™: {max_label_remapped} >= {num_classes}"
            )
        
        # ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèªã¨ä¿®æ­£
        y_processed = y_remapped.astype(np.int64)  # CUDAç”¨ã®å‹ã«å¤‰æ›
        print(f"  å‡¦ç†å¾Œãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿å‹: {y_processed.dtype}")
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®æƒ…å ±
        if confidence_scores is not None:
            print(f"ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘: æœ‰åŠ¹")
            print(f"  ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢å½¢çŠ¶: {confidence_scores.shape}")
            print(f"  å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidence_scores):.3f}")
        else:
            print(f"ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘: ç„¡åŠ¹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰² (å†ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸy_processedã‚’ä½¿ç”¨)
            if confidence_scores is not None:
                X_train_raw, X_test_raw, y_train, y_test, conf_train, conf_test = train_test_split(
                    X, y_processed, confidence_scores, test_size=0.2, random_state=42, stratify=y_processed
                )
                X_train_raw, X_val_raw, y_train, y_val, conf_train, conf_val = train_test_split(
                    X_train_raw, y_train, conf_train, test_size=0.2, random_state=42, stratify=y_train
                )
            else:
                X_train_raw, X_test_raw, y_train, y_test = train_test_split(
                    X, y_processed, test_size=0.2, random_state=42, stratify=y_processed
                )
                X_train_raw, X_val_raw, y_train, y_val = train_test_split(
                    X_train_raw, y_train, test_size=0.2, random_state=42, stratify=y_train
                )
                conf_train = conf_val = conf_test = None

            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®å­¦ç¿’ã¨é©ç”¨
            self.scaler = StandardScaler()
            
            # X_train_raw ã®å½¢çŠ¶ã‚’ (ã‚µãƒ³ãƒ—ãƒ«æ•° * ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·, ç‰¹å¾´é‡æ•°) ã«å¤‰å½¢ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’å­¦ç¿’
            nsamples_train, nx_train, ny_train = X_train_raw.shape
            X_train_reshaped = X_train_raw.reshape((nsamples_train * nx_train, ny_train))
            self.scaler.fit(X_train_reshaped)
            
            # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’é©ç”¨
            X_train = self.scaler.transform(X_train_reshaped).reshape(nsamples_train, nx_train, ny_train)
            
            nsamples_val, nx_val, ny_val = X_val_raw.shape
            X_val_reshaped = X_val_raw.reshape((nsamples_val * nx_val, ny_val))
            X_val = self.scaler.transform(X_val_reshaped).reshape(nsamples_val, nx_val, ny_val)
            
            nsamples_test, nx_test, ny_test = X_test_raw.shape
            X_test_reshaped = X_test_raw.reshape((nsamples_test * nx_test, ny_test))
            X_test = self.scaler.transform(X_test_reshaped).reshape(nsamples_test, nx_test, ny_test)

            print(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
            print(f"  å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
            print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape}")
            print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        input_size = X.shape[2]
        enable_confidence_weighting = confidence_scores is not None
        
        # ã‚¯ãƒ©ã‚¹æ•°ã‚’å®Ÿéš›ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«æ•°ã«è¨­å®š (å†ãƒãƒƒãƒ”ãƒ³ã‚°å¾Œã®ã‚¯ãƒ©ã‚¹æ•°)
        actual_num_classes = num_classes 
        print(f"ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  å…¥åŠ›ã‚µã‚¤ã‚º: {input_size}")
        print(f"  ã‚¯ãƒ©ã‚¹æ•°: {actual_num_classes}")
        
        self.model = self.build_model(input_size, actual_num_classes, model_type, enable_confidence_weighting)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
        train_loader, val_loader, test_loader = self._prepare_data_loaders(
            X_train, y_train, X_val, y_val, X_test, y_test,
            conf_train, conf_val, conf_test
        )
        
        # å­¦ç¿’å®Ÿè¡Œ
        print("ğŸš€ å­¦ç¿’é–‹å§‹...")
        history = self._train_loop(train_loader, val_loader)
        
        # ãƒ†ã‚¹ãƒˆè©•ä¾¡
        test_results = self._evaluate_test(test_loader)
        
        self.history = history
        
        return {
            'test_accuracy': test_results['accuracy'],
            'test_loss': test_results['loss'],
            'f1_score': test_results['f1'],
            'y_test': test_results['y_true'],
            'y_pred': test_results['y_pred'],
            'y_pred_proba': test_results['y_proba'],
            'model_type': model_type,
            'gpu_used': GPU_AVAILABLE,
            'confidence_weighting_used': enable_confidence_weighting,
            'history': history
        }
    
    def _prepare_data_loaders(self, X_train, y_train, X_val, y_val, X_test, y_test,
                             conf_train=None, conf_val=None, conf_test=None):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™"""
        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)
        y_train_tensor = torch.LongTensor(y_train).to(DEVICE)
        X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)
        y_val_tensor = torch.LongTensor(y_val).to(DEVICE)
        X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)
        y_test_tensor = torch.LongTensor(y_test).to(DEVICE)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        if conf_train is not None:
            conf_train_tensor = torch.FloatTensor(conf_train).to(DEVICE)
            conf_val_tensor = torch.FloatTensor(conf_val).to(DEVICE)
            conf_test_tensor = torch.FloatTensor(conf_test).to(DEVICE)
            
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor, conf_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor, conf_val_tensor)
            test_dataset = TensorDataset(X_test_tensor, y_test_tensor, conf_test_tensor)
        else:
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, drop_last=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, drop_last=False)
        
        return train_loader, val_loader, test_loader
    
    def _train_loop(self, train_loader, val_loader):
        """å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-4)
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
        scaler = GradScaler() if GPU_AVAILABLE else None
        
        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        patience = 25 if GPU_AVAILABLE else 15
        epochs = 150 if GPU_AVAILABLE else 100
        
        for epoch in range(epochs):
            # å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º
            train_loss, train_acc = self._train_epoch(train_loader, criterion, optimizer, scaler)
            
            # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º
            val_loss, val_acc = self._validate_epoch(val_loader, criterion)
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            scheduler.step(val_loss)
            
            # å±¥æ­´ä¿å­˜
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['lr'].append(optimizer.param_groups[0]['lr'])
            
            # æ—©æœŸåœæ­¢ãƒã‚§ãƒƒã‚¯
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"æ—©æœŸåœæ­¢: {patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
                    break
            
            if (epoch + 1) % 10 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Train: {train_acc:.4f}, Val: {val_acc:.4f}')
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
        
        return history
    
    def _train_epoch(self, train_loader, criterion, optimizer, scaler):
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, batch_data in enumerate(train_loader):
            try:
                if len(batch_data) == 3:
                    batch_X, batch_y, batch_conf = batch_data
                else:
                    batch_X, batch_y = batch_data
                    batch_conf = None
                
                # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
                if torch.any(batch_y < 0) or torch.any(batch_y >= self.model.num_classes):
                    print(f"âš ï¸  ãƒãƒƒãƒ {batch_idx}: ç„¡åŠ¹ãªãƒ©ãƒ™ãƒ«å€¤ã‚’æ¤œå‡º")
                    print(f"   ãƒ©ãƒ™ãƒ«ç¯„å›²: {torch.min(batch_y)} ï½ {torch.max(batch_y)}")
                    print(f"   æœŸå¾…ç¯„å›²: 0 ï½ {self.model.num_classes - 1}")
                    continue  # ã“ã®ãƒãƒƒãƒã‚’ã‚¹ã‚­ãƒƒãƒ—
                
                optimizer.zero_grad()
                
                if GPU_AVAILABLE and scaler:
                    with autocast():
                        outputs = self.model(batch_X, batch_conf)
                        loss = criterion(outputs, batch_y)
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = self.model(batch_X, batch_conf)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
                
            except RuntimeError as e:
                if "device-side assert" in str(e):
                    print(f"âŒ CUDAã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {batch_idx}): {e}")
                    print(f"   ãƒãƒƒãƒãƒ©ãƒ™ãƒ«æƒ…å ±:")
                    print(f"     å½¢çŠ¶: {batch_y.shape}")
                    print(f"     ãƒ‡ãƒ¼ã‚¿å‹: {batch_y.dtype}")
                    print(f"     å€¤: {batch_y.cpu().numpy()}")
                    print(f"     ç¯„å›²: {torch.min(batch_y)} ï½ {torch.max(batch_y)}")
                    raise e
                else:
                    print(f"âŒ å­¦ç¿’ã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {batch_idx}): {e}")
                    raise e
        
        if total == 0:
            print("âš ï¸  æœ‰åŠ¹ãªãƒãƒƒãƒãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return 0.0, 0.0
        
        return total_loss / len(train_loader), correct / total
    
    def _validate_epoch(self, val_loader, criterion):
        """1ã‚¨ãƒãƒƒã‚¯ã®æ¤œè¨¼"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch_data in val_loader:
                if len(batch_data) == 3:
                    batch_X, batch_y, batch_conf = batch_data
                else:
                    batch_X, batch_y = batch_data
                    batch_conf = None
                
                outputs = self.model(batch_X, batch_conf)
                loss = criterion(outputs, batch_y)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
        
        return total_loss / max(1, len(val_loader)), correct / total
    
    def _evaluate_test(self, test_loader):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡"""
        self.model.eval()
        criterion = nn.CrossEntropyLoss()
        total_loss = 0.0
        all_predictions = []
        all_probabilities = []
        all_true_labels = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                if len(batch_data) == 3:
                    batch_X, batch_y, batch_conf = batch_data
                else:
                    batch_X, batch_y = batch_data
                    batch_conf = None
                
                outputs = self.model(batch_X, batch_conf)
                loss = criterion(outputs, batch_y)
                
                total_loss += loss.item()
                probabilities = F.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
                all_true_labels.extend(batch_y.cpu().numpy())
        
        y_true = np.array(all_true_labels)
        y_pred = np.array(all_predictions)
        
        return {
            'loss': total_loss / max(1, len(test_loader)),
            'accuracy': np.mean(y_true == y_pred),
            'f1': f1_score(y_true, y_pred, average='weighted'),
            'y_true': y_true,
            'y_pred': y_pred,
            'y_proba': np.array(all_probabilities)
        }
    
    def compare_models(self, X: np.ndarray, y: np.ndarray, confidence_scores: np.ndarray = None) -> Dict:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ"""
        model_types = ["simple", "bidirectional", "stacked"]
        results = {}
        
        for model_type in model_types:
            print(f"\n--- {model_type.upper()} LSTM ---")
            try:
                result = self.train_model(X, y, confidence_scores, model_type)
                results[model_type] = result
                print(f"{model_type} å®Œäº†: ç²¾åº¦={result['test_accuracy']:.4f}")
            except Exception as e:
                print(f"âŒ {model_type} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                results[model_type] = {'error': str(e)}
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ
        valid_results = {name: result for name, result in results.items() 
                        if 'error' not in result}
        
        if valid_results:
            best_model = max(valid_results.keys(), 
                           key=lambda x: valid_results[x]['test_accuracy'])
            results['best_model'] = best_model
            print(f"\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model.upper()}")
        
        return results
    
    def plot_training_history(self):
        """å­¦ç¿’å±¥æ­´ã®å¯è¦–åŒ–"""
        if self.history is None:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æå¤±
        axes[0, 0].plot(self.history['train_loss'], label='Training Loss')
        axes[0, 0].plot(self.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # ç²¾åº¦
        axes[0, 1].plot(self.history['train_acc'], label='Training Accuracy')
        axes[0, 1].plot(self.history['val_acc'], label='Validation Accuracy')
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å­¦ç¿’ç‡
        axes[1, 0].plot(self.history['lr'])
        axes[1, 0].set_title('Learning Rate')
        axes[1, 0].set_yscale('log')
        axes[1, 0].grid(True)
        
        # çµ±è¨ˆæƒ…å ±
        final_train_acc = self.history['train_acc'][-1]
        final_val_acc = self.history['val_acc'][-1]
        axes[1, 1].text(0.1, 0.7, f'Final Train Acc: {final_train_acc:.4f}', fontsize=12)
        axes[1, 1].text(0.1, 0.5, f'Final Val Acc: {final_val_acc:.4f}', fontsize=12)
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(self.models_dir / f'lstm_training_history_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ–"""
        cm = confusion_matrix(y_true, y_pred)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ©ãƒ™ãƒ«åã‚’ä½¿ç”¨
        display_labels = self.active_phase_labels if self.active_phase_labels else self.phase_labels
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=display_labels, yticklabels=display_labels)
        plt.title('LSTM Model - Confusion Matrix')
        plt.xlabel('Predicted Phase')
        plt.ylabel('True Phase')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(self.models_dir / f'lstm_confusion_matrix_{timestamp}.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def evaluate_model(self, results: Dict):
        """è©³ç´°ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        print(f"\n=== è©³ç´°è©•ä¾¡ ===")
        
        y_test = results['y_test']  # ã“ã‚Œã‚‰ã¯å†ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«
        y_pred = results['y_pred']  # ã“ã‚Œã‚‰ã¯å†ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ (ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ©ãƒ™ãƒ«åã¨å†ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ç¯„å›²ã‚’ä½¿ç”¨)
        report_str = ""
        if self.active_phase_labels:
            # labelså¼•æ•°ã§ãƒ¬ãƒãƒ¼ãƒˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚’æŒ‡å®š (0ã‹ã‚‰å§‹ã¾ã‚‹é€£ç¶šã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
            report_labels = np.arange(len(self.active_phase_labels))
            print("åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
            report_str = classification_report(y_test, y_pred, target_names=self.active_phase_labels, labels=report_labels, zero_division=0)
            print(report_str)
        else:
            print("åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ):")
            report_str = classification_report(y_test, y_pred, zero_division=0)
            print(report_str)

        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = self.models_dir / f'classification_report_{timestamp}.txt'
        try:
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(f"Classification Report ({timestamp})\n")
                f.write("=" * 30 + "\n")
                f.write(f"Model Type: {results.get('model_type', 'N/A')}\n")
                f.write(f"Test Accuracy: {results.get('test_accuracy', 0.0):.4f}\n")
                f.write(f"F1 Score (Weighted): {results.get('f1_score', 0.0):.4f}\n")
                f.write("=" * 30 + "\n\n")
                f.write(report_str)
            print(f"âœ… åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_filename.name}")
        except Exception as e:
            print(f"âš ï¸ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

        # å¯è¦–åŒ–
        self.plot_confusion_matrix(y_test, y_pred)
        self.plot_training_history()
    
    def save_model(self, results: Dict, feature_names: List[str], video_names: List[str] = None) -> Dict[str, str]:
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å‹•ç”»åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        if video_names:
            video_part = video_names[0] if len(video_names) == 1 else "multi"
            video_part = "".join(c for c in video_part if c.isalnum() or c in "._-")
            video_part = video_part[:20] if len(video_part) > 20 else video_part
        else:
            video_part = "unknown"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        model_path = self.models_dir / f"tennis_pytorch_model_{video_part}_{timestamp}.pth"
        scaler_path = self.models_dir / f"tennis_pytorch_scaler_{video_part}_{timestamp}.pkl"
        metadata_path = self.models_dir / f"tennis_pytorch_metadata_{video_part}_{timestamp}.json"
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.input_size,
                'hidden_sizes': self.model.hidden_sizes,
                'num_classes': self.model.num_classes,
                'dropout_rate': self.model.dropout_rate,
                'model_type': self.model.model_type,
                'use_batch_norm': getattr(self.model, 'use_batch_norm', True),
                'enable_confidence_weighting': getattr(self.model, 'enable_confidence_weighting', False)
            }
        }, model_path)
        
        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        joblib.dump(self.scaler, scaler_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            'creation_time': timestamp,
            'video_names': video_names or [],
            'framework': 'pytorch',
            'model_type': results.get('model_type', 'bidirectional'),
            'test_accuracy': float(results['test_accuracy']),
            'f1_score': float(results['f1_score']),
            'phase_labels': self.active_phase_labels if self.active_phase_labels else self.phase_labels, # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
            'label_map': self.label_map, # ãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ã‚’è¿½åŠ 
            'feature_names': feature_names,
            'sequence_length': int(self.sequence_length), # Pythonã®intå‹ã«å¤‰æ›
            'device': str(DEVICE),
            'gpu_used': results.get('gpu_used', False)
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\n=== ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº† ===")
        print(f"ãƒ¢ãƒ‡ãƒ«: {model_path.name}")
        print(f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {scaler_path.name}")
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata_path.name}")
        
        return {
            'model_path': str(model_path),
            'scaler_path': str(scaler_path),
            'metadata_path': str(metadata_path)
        }
    
    def train_complete_pipeline(self, csv_path: str = None, model_comparison: bool = True, 
                               quick_mode: bool = False, sample_ratio: float = 1.0) -> bool:
        """å®Œå…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        print("=== ãƒ†ãƒ‹ã‚¹å±€é¢åˆ†é¡LSTMå­¦ç¿’é–‹å§‹ ===")
        
        if quick_mode:
            print("ğŸš€ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
            self.sequence_length = 15
            self.lstm_units = [64, 32] if not GPU_AVAILABLE else [128, 64]
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df, success = self.load_dataset(csv_path, sample_ratio)
        if not success:
            return False
        
        video_names = df['video_name'].unique().tolist() if 'video_name' in df.columns else []
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
        try:
            X, y, confidence_scores, feature_names = self.prepare_sequences(df)
        except Exception as e:
            print(f"âŒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        try:
            if model_comparison and not quick_mode:
                all_results = self.compare_models(X, y, confidence_scores)
                if 'best_model' in all_results:
                    best_model_type = all_results['best_model']
                    results = self.train_model(X, y, confidence_scores, best_model_type)
                else:
                    return False
            else:
                model_type = "simple" if quick_mode else "bidirectional"
                results = self.train_model(X, y, confidence_scores, model_type)
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        
        # è©•ä¾¡ã¨ä¿å­˜
        try:
            self.evaluate_model(results)
            self.save_model(results, feature_names, video_names)
            print(f"\nğŸ‰ LSTMå­¦ç¿’å®Œäº†ï¼")
            return True
        except Exception as e:
            print(f"âŒ è©•ä¾¡ãƒ»ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def check_cuda_installation():
    """CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯"""
    print("=== CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯ ===")
    print(f"PyTorchç‰ˆ: {torch.__version__}")
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {'ã¯ã„' if torch.cuda.is_available() else 'ã„ã„ãˆ'}")
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"æ¤œå‡ºGPUæ•°: {device_count}")
        
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
    
    return torch.cuda.is_available()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("=== ãƒ†ãƒ‹ã‚¹å‹•ç”»å±€é¢åˆ†é¡PyTorch LSTMå­¦ç¿’ãƒ„ãƒ¼ãƒ« ===")
    
    check_cuda_installation()
    trainer = TennisLSTMTrainer()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    dataset_files = []
    patterns = ["tennis_features_dataset_*.csv", "tennis_features_*.csv"]
    
    for pattern in patterns:
        dataset_files.extend(list(trainer.features_dir.glob(pattern)))
    
    if not dataset_files:
        for pattern in patterns:
            dataset_files.extend(list(trainer.training_data_dir.glob(pattern)))
    
    # é‡è¤‡ã‚’é™¤å»
    dataset_files = list(set(dataset_files))
    
    if not dataset_files:
        print("\nâŒ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: tennis_features_dataset_*.csv, tennis_features_*.csv")
        print(f"æ¤œç´¢å ´æ‰€: {trainer.features_dir}, {trainer.training_data_dir}")
        return
    
    print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {len(dataset_files)}ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º")
    
    # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    print(f"\nå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é¸æŠ:")
    print(f"1: é€šå¸¸å­¦ç¿’ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚ã‚Šï¼‰")
    print(f"2: é«˜é€Ÿå­¦ç¿’ï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰")
    print(f"3: ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠå­¦ç¿’")
    print(f"4: æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å­¦ç¿’")
    print(f"5: çµ‚äº†")
    
    try:
        choice = input("\né¸æŠã—ã¦ãã ã•ã„ (1-5): ").strip()
        
        config = {
            '1': (None, False, True, 1.0),
            '2': (None, True, False, 0.3),
            '3': (None, False, True, 1.0),
            '4': (str(max(dataset_files, key=lambda x: x.stat().st_mtime)), False, True, 1.0),
            '5': None
        }
        
        if choice == '5':
            return
        
        if choice not in config:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return
        
        selected_file, quick_mode, model_comparison, sample_ratio = config[choice]
        
        # å­¦ç¿’å®Ÿè¡Œ
        success = trainer.train_complete_pipeline(
            csv_path=selected_file,
            model_comparison=model_comparison,
            quick_mode=quick_mode,
            sample_ratio=sample_ratio
        )
        
        if success:
            print(f"\nâœ… å…¨ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        else:
            print(f"\nâŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            
    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
