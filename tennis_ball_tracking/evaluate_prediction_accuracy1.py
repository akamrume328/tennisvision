# evaluate_prediction_accuracy.py

import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import joblib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Generator
import argparse
import warnings

# æ©Ÿæ¢°å­¦ç¿’ãƒ»è©•ä¾¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ã‚°ãƒ©ãƒ•æç”»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# ===== GPUè¨­å®š (predict_lstm_model_cv.py ã‹ã‚‰æµç”¨) =====
def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"GPUæ¤œå‡º: {device_count}å°")
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        return torch.device('cuda')
    else:
        print("è­¦å‘Š: GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
        return torch.device('cpu')

DEVICE = setup_gpu_config()

# ===== ãƒ¢ãƒ‡ãƒ«å®šç¾© (predict_lstm_model_cv.py ã¨å®Œå…¨ã«åŒä¸€) =====
class TennisLSTMModel(nn.Module):
    """PyTorch LSTM ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’æ™‚ã¨å®Œå…¨äº’æ›ï¼‰"""
    
    def __init__(self, input_size: int, hidden_sizes: List[int], num_classes: int, 
                 dropout_rate: float, model_type: str, use_batch_norm: bool, 
                 enable_confidence_weighting: bool):
        super(TennisLSTMModel, self).__init__()
        
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.num_classes = num_classes
        self.dropout_rate = dropout_rate
        self.model_type = model_type
        self.use_batch_norm = use_batch_norm
        self.enable_confidence_weighting = enable_confidence_weighting
        
        if self.enable_confidence_weighting:
            self.confidence_attention = nn.Sequential(
                nn.Linear(input_size, hidden_sizes[0] // 4),
                nn.ReLU(),
                nn.Linear(hidden_sizes[0] // 4, 1),
                nn.Sigmoid()
            )
        
        self._build_lstm_layers()
        
        self.dropout = nn.Dropout(dropout_rate)
        self.fc1 = nn.Linear(self.lstm_output_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc_out = nn.Linear(64, num_classes)
        
        if self.use_batch_norm:
            self.bn1 = nn.BatchNorm1d(128)
            self.bn2 = nn.BatchNorm1d(64)
        else:
            self.ln1 = nn.LayerNorm(128)
            self.ln2 = nn.LayerNorm(64)
        
        self._init_weights()
    
    def _build_lstm_layers(self):
            num_layers_to_build = len(self.hidden_sizes)
            self.lstm = nn.LSTM(
                input_size=self.input_size,
                hidden_size=self.hidden_sizes[0],
                num_layers=num_layers_to_build,
                batch_first=True,
                dropout=self.dropout_rate if num_layers_to_build > 1 else 0,
                bidirectional=(self.model_type == "bidirectional")
            )
            if self.model_type == "bidirectional":
                self.lstm_output_size = self.hidden_sizes[0] * 2
            else:
                self.lstm_output_size = self.hidden_sizes[0]

    def _init_weights(self):
        for name, param in self.named_parameters():
            if 'lstm' in name:
                if 'weight_ih' in name: nn.init.xavier_uniform_(param)
                elif 'weight_hh' in name: nn.init.orthogonal_(param)
                elif 'bias' in name: nn.init.zeros_(param)
            elif 'fc' in name or 'confidence_attention' in name:
                if 'weight' in name: nn.init.xavier_uniform_(param)
                elif 'bias' in name: nn.init.zeros_(param)

    def forward(self, x, confidence_scores=None):
        if self.enable_confidence_weighting and confidence_scores is not None:
            attention_weights = self.confidence_attention(x).squeeze(-1)
            combined_weights = attention_weights * confidence_scores
            x = x * combined_weights.unsqueeze(-1)
        
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, -1, :]
        lstm_out = self.dropout(lstm_out)
        
        out = F.relu(self.fc1(lstm_out))
        out = self._apply_normalization(out, self.bn1 if self.use_batch_norm else self.ln1)
        out = self.dropout(out)
        
        out = F.relu(self.fc2(out))
        out = self._apply_normalization(out, self.bn2 if self.use_batch_norm else self.ln2)
        out = self.dropout(out)
        
        return self.fc_out(out)

    def _apply_normalization(self, x, norm_layer):
        if self.use_batch_norm:
            return norm_layer(x) if x.size(0) > 1 else x
        return norm_layer(x)

# ===== ç²¾åº¦æ¤œè¨¼ã‚¯ãƒ©ã‚¹ =====
class PredictionEvaluator:
    def __init__(self, models_dir: str = "./training_data/lstm_models", 
                 features_dir: str = "./training_data/features"):
        self.models_dir = Path(models_dir)
        self.features_dir = Path(features_dir)
        self.output_dir = Path("./evaluation_results")
        self.output_dir.mkdir(exist_ok=True)

        self.model: Optional[TennisLSTMModel] = None
        self.scaler = None
        self.metadata: Optional[Dict] = None
        self.device = DEVICE
        
        self.phase_labels: List[str] = []
        self.feature_names: List[str] = []
        self.sequence_length: int = 30
        self.label_map: Optional[Dict[str, int]] = None
        self.label_map_inv: Optional[Dict[int, str]] = None
        
        print(f"ç²¾åº¦æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚çµæœã¯ '{self.output_dir}' ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")

    def select_model_files(self) -> Optional[Tuple[Path, Path, Path]]:
        """å¯¾è©±å½¢å¼ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ (predict_lstm_model_cv.pyã‹ã‚‰æµç”¨)"""
        print(f"\n=== 1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        all_model_files = sorted(list(self.models_dir.glob("**/tennis_pytorch*.pth")), key=lambda p: p.stat().st_mtime, reverse=True)
        if not all_model_files:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (*.pth) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {self.models_dir}")
            return None

        valid_sets = []
        for mf_path in all_model_files:
            if not mf_path.name.endswith("_model.pth"): continue
            base_name = mf_path.name.removesuffix("_model.pth")
            scaler_path = mf_path.parent / f"{base_name}_scaler.pkl"
            meta_path = mf_path.parent / f"{base_name}_metadata.json"
            if scaler_path.exists() and meta_path.exists():
                valid_sets.append((mf_path, scaler_path, meta_path))

        if not valid_sets:
            print(f"âŒ å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆï¼ˆscaler, metadataãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        for i, (mf_path, _, _) in enumerate(valid_sets, 1):
            print(f"  {i}. {mf_path.relative_to(self.models_dir)} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(mf_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(valid_sets)}): ").strip()
            return valid_sets[int(choice) - 1]
        except (ValueError, IndexError):
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def load_model_and_metadata(self, model_path: Path, scaler_path: Path, metadata_path: Path) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã¨é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ (predict_lstm_model_cv.pyã‹ã‚‰æµç”¨ãƒ»ä¿®æ­£)"""
        print(f"\n--- ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---")
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f: self.metadata = json.load(f)
            self.scaler = joblib.load(scaler_path)
            
            model_config = self.metadata['model_config']
            self.model = TennisLSTMModel(
                input_size=len(self.metadata['feature_names']),
                num_classes=len(self.metadata['phase_labels']),
                hidden_sizes=model_config['lstm_units'],
                dropout_rate=model_config.get('dropout_rate', 0.3),
                model_type=model_config.get('model_type', 'bidirectional'),
                use_batch_norm=model_config.get('batch_size', 64) > 1,
                enable_confidence_weighting=model_config.get('enable_confidence_weighting', False)
            )
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.to(self.device)
            self.model.eval()

            self.phase_labels = self.metadata['phase_labels']
            self.feature_names = self.metadata['feature_names']
            self.sequence_length = model_config.get('sequence_length', 30)
            
            # ãƒ©ãƒ™ãƒ«ã¨IDã®å¯¾å¿œè¾æ›¸ã‚’ä½œæˆ
            self.label_map = {name: i for i, name in enumerate(self.phase_labels)}
            self.label_map_inv = {i: name for i, name in enumerate(self.phase_labels)}
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path.name}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«/ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback; traceback.print_exc()
            return False

    def select_feature_file(self) -> Optional[Path]:
        """å¯¾è©±å½¢å¼ã§ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ"""
        print(f"\n=== 2. ç²¾åº¦æ¤œè¨¼ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        print(f"'{self.features_dir}' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¢ã—ã¾ã™...")
        
        # 'tennis_features_*.csv' (ãƒ©ãƒ™ãƒ«ä»˜ã) ã‚’å„ªå…ˆçš„ã«æ¢ã™
        feature_files = sorted(list(self.features_dir.glob("tennis_features_*.csv")), key=lambda p: p.stat().st_mtime, reverse=True)
        
        if not feature_files:
            print(f"âŒ æ­£è§£ãƒ©ãƒ™ãƒ«ä»˜ãã®ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ« (tennis_features_*.csv) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        for i, f_path in enumerate(feature_files, 1):
            print(f"  {i}. {f_path.name} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(f_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(feature_files)}): ").strip()
            return feature_files[int(choice) - 1]
        except (ValueError, IndexError):
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def _generate_sequences(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray]) -> Generator:
        """æ¨è«–ç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        num_frames = X_scaled.shape[0]
        if num_frames < self.sequence_length:
            return
        for i in range(num_frames - self.sequence_length + 1):
            seq_X = X_scaled[i : i + self.sequence_length]
            original_idx = i + self.sequence_length - 1
            seq_conf = confidence_scores[i : i + self.sequence_length] if confidence_scores is not None else None
            yield seq_X, original_idx, seq_conf

    def predict(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray], batch_size: int = 256) -> Optional[Tuple]:
        """æ¨è«–ã‚’å®Ÿè¡Œ (predict_lstm_model_cv.pyã‹ã‚‰æµç”¨)"""
        if not self.model: return None
        print(f"\n--- æ¨è«–å®Ÿè¡Œ ---")
        
        all_preds, all_probas, all_indices = [], [], []
        seq_generator = self._generate_sequences(X_scaled, confidence_scores)
        
        batch_seq, batch_conf, batch_idx = [], [], []
        for seq_X, original_idx, seq_conf in seq_generator:
            batch_seq.append(seq_X)
            batch_idx.append(original_idx)
            if seq_conf is not None: batch_conf.append(seq_conf)

            if len(batch_seq) >= batch_size:
                self._process_batch(batch_seq, batch_conf, all_preds, all_probas)
                all_indices.extend(batch_idx)
                batch_seq, batch_conf, batch_idx = [], [], []

        if batch_seq:
            self._process_batch(batch_seq, batch_conf, all_preds, all_probas)
            all_indices.extend(batch_idx)

        print(f"âœ… æ¨è«–å®Œäº†: {len(all_preds)}ä»¶")
        return np.array(all_preds), np.array(all_probas), all_indices

    def _process_batch(self, batch_seq, batch_conf, all_preds, all_probas):
        """ãƒãƒƒãƒå˜ä½ã§æ¨è«–å‡¦ç†"""
        X_tensor = torch.from_numpy(np.array(batch_seq, dtype=np.float32)).to(self.device)
        conf_tensor = torch.from_numpy(np.array(batch_conf, dtype=np.float32)).to(self.device) if batch_conf else None
        
        with torch.no_grad():
            outputs = self.model(X_tensor, conf_tensor)
            probas = F.softmax(outputs, dim=1)
            _, preds = torch.max(probas, 1)
        
        all_preds.extend(preds.cpu().numpy())
        all_probas.extend(probas.cpu().numpy())

    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[str]):
        """ç²¾åº¦ã®è©•ä¾¡ã¨çµæœè¡¨ç¤º"""
        print("\n===== 3. ç²¾åº¦æ¤œè¨¼çµæœ =====")
        
        # æ­£è§£ç‡
        accuracy = accuracy_score(y_true, y_pred)
        print(f"\næ­£è§£ç‡ (Accuracy): {accuracy:.4f}")
        
        # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
        print("\nåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        report = classification_report(y_true, y_pred, target_names=labels, zero_division=0)
        print(report)
        
        # æ··åŒè¡Œåˆ—
        self.plot_confusion_matrix(y_true, y_pred, labels)

    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, labels: List[str]):
        """æ··åŒè¡Œåˆ—ã®å¯è¦–åŒ– (train_lstm_model_cv.pyã‹ã‚‰æµç”¨)"""
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted Phase')
        plt.ylabel('True Phase')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        save_path = self.output_dir / f'evaluation_confusion_matrix_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        plt.savefig(save_path, dpi=300)
        print(f"\næ··åŒè¡Œåˆ—ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
        plt.show()

    def run_evaluation_pipeline(self):
        """ç²¾åº¦æ¤œè¨¼ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        # 1. ãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_files = self.select_model_files()
        if not selected_files: return
        
        if not self.load_model_and_metadata(*selected_files): return

        # 2. ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        input_csv_path = self.select_feature_file()
        if not input_csv_path: return

        # 3. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print(f"\n--- ãƒ‡ãƒ¼ã‚¿æº–å‚™: {input_csv_path.name} ---")
        try:
            df = pd.read_csv(input_csv_path)
            # æ­£è§£ãƒ©ãƒ™ãƒ«åˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            if 'label' not in df.columns:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã«æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’ç¤ºã™ 'label' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                return
            
            X_df = df[self.feature_names].copy().fillna(0).replace([np.inf, -np.inf], 0)
            X_scaled = self.scaler.transform(X_df).astype(np.float32)
            
            confidence_scores = None
            if self.model and self.model.enable_confidence_weighting:
                confidence_scores = df.get('interpolation_confidence', pd.Series(np.ones(len(df)))).fillna(1.0).astype(np.float32).values

        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return
            
        # 4. æ¨è«–å®Ÿè¡Œ
        pred_ids, _, pred_indices = self.predict(X_scaled, confidence_scores)
        if pred_ids is None: return

        # 5. æ­£è§£ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®æº–å‚™
        # æ¨è«–ãŒè¡Œã‚ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã«å¯¾å¿œã™ã‚‹æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        true_ids = df.loc[pred_indices, 'label'].values
        
        # IDã‚’ãƒ©ãƒ™ãƒ«åã«å¤‰æ›
        y_true_labels = [self.label_map_inv.get(int(i), "Unknown") for i in true_ids]
        y_pred_labels = [self.label_map_inv.get(i, "Unknown") for i in pred_ids]

        # 6. ç²¾åº¦è©•ä¾¡
        self.evaluate(y_true_labels, y_pred_labels, self.phase_labels)
        
        print("\nğŸ‰ ç²¾åº¦æ¤œè¨¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    parser.add_argument('--models_dir', type=str, default="./training_data/lstm_models", help="ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument('--features_dir', type=str, default="./training_data/features", help="ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    args = parser.parse_args()

    try:
        evaluator = PredictionEvaluator(models_dir=args.models_dir, features_dir=args.features_dir)
        evaluator.run_evaluation_pipeline()
    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()